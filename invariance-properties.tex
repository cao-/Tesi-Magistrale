\def\CA{\mathbox{$\boldmath\cal A$}}
\def\CC{\mathbox{$\boldmath\cal C$}}

%First basic properties, like 0<Q<N, Q=1-Ns/Nsk, ...

We now study the properties satisfied by the $\phi$-regularity vector~$\Bm Q$\,, and hence by the classifier~$\Cal R$ built upon it.  In what follows $\phi$ will denote again one of the positive definite functions introduced in Section~\ref[positivesec], and~$\phi_ε$ its $ε$-scaled version as defined in~\ref[phiepsilon].  Each considered quantity may be dependent on~$ε$, but for simplicity of notation this dependence won't be explicitly indicated.

Let $X = \{\range x_N/\}\subset\Omega\subseteq\R^d\!\.$ be a set of data sites, and $F=\{\range f_N/\}$ a set of associated values sampled from an unknown function~$f$.  We are interested in seeing how the vector~$\Bm Q$ computed from~$(X, F)$ changes when a {\em linear transformation} is applied to~$F$ or, equivalently, to the function~$f$.  Assume that $f$ is transformed into~$g = γ f + η$ for some~$γ, η\in\R$, so that $F$ is transformed into
$$
G = \{\.\range g_N/\} 
  = \{\.γ f_1 + η,\, γ f_2 + η,\, \dots,\, γ f_N + η\}
  \eqcolon γ F + η.
$$
If $X$ is fixed, in order to distinguish the interpolant of $(X,F)$ from the interpolant of~$(X, G)$ we'll denote them as $s_{F}$ and~$s_{G\!}$ respectively; the same for the vector~$\Bm Q\,$, which will appear as $\Bm Q_{\.F\!}$ in the first case and~$\Bm Q_{\.G\!}$ in the second case; and also possibly for any other quantity that changes when $F$ becomes~$G$.


The easier case~$η=0$ gets analysed first. The following property is the main reason why, in the first place, we didn't consider the vector~$\Bm U$ as is, but rather switched to its ``normalised'' version~$\Bm Q$.

\label[scaleprop]
\preskip
\property If $G = γF$ for some nonzero~$γ\in\R$, then~$\Bm Q_{\.G\!} = \Bm Q_{\.F}$.

\proof
If~$f_k = 0$ for all~$k\in\{1,2,\dots,N\}$, then also $g_k= 0$ for all~$k$, and in this case both $\Bm Q_{\.F\!}$ and~$\Bm Q_{\.G\!}$ are the zero vector.
Otherwise, since the interpolation process is linear, we know that $s_G=s_{γF}= γ s_F$ and, similarly, $s_G^{(k)\!}=s_{γF}^{(k)\!}=γs_F^{(k)\!}$. Thus, for each index~$k\in\{1,2,\dots,N\}$, the $k$-th element of~$\Bm Q_{\.G\!}$ assumes the value
$$
(Q_G)_k = {\norm{s_G}_\phi^2 - \norm{s_G^{(k)\!}}_\phi^2 \over\norm{s_G}_\phi^2}
        = 1-{\norm{s_G^{(k)\!}}_\phi^2\over\norm{s_G}_\phi^2}
        = 1-{\norm{γs_F^{(k)\!}}_\phi^2\over\norm{γs_F}_\phi^2}
        = 1-{γ^2\norm{s_F^{(k)\!}}_\phi^2\over γ^2\norm{s_F}_\phi^2}
        = (Q_F)_k.~\QED
$$
\postskip


Then it remains to analyse the case~$η\neq 0$.  Unfortunately, it can be shown that the vector~$\Bm Q$ is {\em not} invariant under a translation of the function values~$F$.  But we'll talk about it later, only after the next result.

\label[flatlimit]
\preskip
\property  For a fixed $F$ the vector $\Bm Q\,= \Bm Q_{\.F\!}\,$, which depends on the shape parameter~$ε$, converges to a limit vector as $ε→0$.
\proof
Notice that $\norm s_\phi = 0$ either for all~$ε$ or for none. When it is zero, $\Bm Q$~is constantly equal to the zero vector, and its convergence is proved.  If instead $\norm s_\phi\neq 0$, then the elements $Q_k$ of the vector~$\Bm Q$ are defined as~$Q_k = U_k/\smash{\norm s_\phi^2}$. By expanding the scalar product in the denominator and recalling the definition~\ref[vectorE] of the cross-validation vector~$\Bm E$, we can split~$Q_k$ into two parts, namely
$$
Q_k = {U_k\over \norm s_\phi^2} = {U_k \over \Bm α^T \Bm f} = {E_k {α_k \over \Bm α^T \Bm f}} \eqcolon E_k\,W_k,
$$
where, as usual, $\Bm α$~is the vector of coefficients of~$s$ with respect to the basis of kernel translates. If~$\Bm C$ is the inverse of the interpolation matrix~$\Bm A=\Bm A_{\phi, X}$, simple algebraic manipulations lead to the following expressions for $E_k$ and~$W_k$:
$$
%\def\_ineqmark{\_incr\_dnum \_wlabel\_thednum \lower3.5ex\hbox{\_thednum\kern-.080em}}
\eqalignno{
  &E_k = {α_k \over C_{k,k}} = {(\Bm C\. \Bm f\,)_k \over C_{k,k}} =  {\sum_{j=1}^N C_{k,j}\, f_j \over C_{k,k}} = \sum_{j = 1}^N f_j\,\frame{$\displaystyle \hbyw{1.8ex}{0em} C_{k,j} \over \displaystyle C_{k,k}$}\,, & \eqmark[EkWk]\cr
 &\eqalign{W_k = {α_k \over \Bm α^T \Bm f} = {α_k \over \Bm f^{\,\.T}\! \Bm C\, \Bm f} &= {\sum_{j=1}^N C_{k,j}\,f_j \over \sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l}} = \sum_{j=1}^N\. f_j\. {C_{k,j} \over \sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l}}\cr
    &= \sum_{\rlap{$\matrix{j = 1\,\,\cr C_{k,j}\neq 0}$}}^N\, \Biggl({\sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l} \over C_{k,j}}\Biggr)^{-1}\!\! = \sum_{\rlap{$\matrix{j = 1\,\,\cr C_{k,j}\neq 0}$}}^N f_j\,\Bigl(\sum_{i=1}^N\sum_{l=1}^N f_i\, f_l\, \frame{$\displaystyle \hbyw{1.8ex}{0em} C_{i,l} \over \displaystyle C_{k,j}$}\,\Bigr)^{\rlap{$\scriptstyle-1$}}.
}}
$$
The dependence on~$ε$ of the quantities $E_k$ and~$W_k$ lies  only in the framed ratios,  whose numerator and denominator are elements of the matrix~$\Bm C$.  If we show that those terms converge when~$ε→0$, then we are done.

We can fix a radius~$r\geq 0$ and view $\phi_ε(r)$ as a function of~$ε$ rather than~$r$, and consider its power series expansion\fnote{All the positive definite functions that we are considering---the ones described in Section~\ref[positivesec]---are {\em analytic} in a (right) neighbour of~$0$.} centred at the point~$ε=0$,
$$
\phi_ε(r) = \phi(εr) = a_{0\!}(r) + a_{1}\!(r)\,ε + a_{2\!}(r)\,ε^2 + a_{3\!}(r)\,ε^3 + \cdots,
$$
whose coefficients~$a_{n\!}(r)$ are explicit\fnote{They can actually be computed from the expression of~$\phi$. For instance, the first term~$a_{0\!}(r)$ is independent of~$r$ and has value~$\phi(0)$, because $\phi$ is continuous at~$r=0$. The other terms are the derivatives of~$\phi$---the power series of~$\phi$ is indeed its Taylor series.} functions of~$r$.
With $\Bm R$ being the distance matrix associated to~$X$, that is the $N\times N$~matrix of elements~$R_{i,j}\coloneq \norm{x_i - x_j}$, define for each~$n$ the matrix~$\CA_{\Bm n}$ by applying the function $a_n$ to each element of~$\Bm R$, explicitly
$$
(\Cal A_{n})_{i,j}\coloneq a_n(R_{i,j}) = a_n(\norm{x_i - x_j}),\qquad i,j\in\{1,2,\dots,N\}.
$$
Under these definitions, the $ε$-dependent matrix~$\Bm A$ has at the point~$ε=0$ the power series expansion
$$
 \Bm{A} = \CA_{\Bm 0} + \CA_{\Bm 1}\,ε + \CA_{\Bm 2}\,ε^2 + \CA_{\Bm 3}\,ε^3 + \cdots.\eqmark[powerA]
$$



 The inverse~$\Bm C$ of the matrix~$\Bm A$ exists for all~$ε$ and has elements that are rational functions of the elements of~$\Bm A$.  This means that $\Bm C$ has a Laurent series expansion at~$ε=0$,
$$
\Bm C =  \CC_{\!\!\Bm{--p}}\,ε^{-p} + \CC_{\!\!\Bm{--p+}\Bm 1}\,ε^{-p+1} + \CC_{\!\!\Bm{--p+}\Bm 2}\,ε^{-p+2} + \CC_{\!\!\Bm{--p+}\Bm 3}\,ε^{-p+3} + \cdots, \eqmark[laurentC]
$$
where $p$ is the order of singularity at $ε=0$ of the matrix~$\Bm C\.$ (Refer to Langenhop~\cite[langenhop_1971] for a general treatment of this topic, and also to Gonzales-Rodriguez, Moscoso and Kindelan~\cite[gonzales_rodriguez-moscoso-kindelan_2015] for a specific application in the context of radial basis functions).
This means that near~$ε=0$ the matrix~$\Bm C$ is just like the first term~$\CC_{\!\!\Bm{--p}}\,ε^{-p\!}$ of its Laurent series, therefore the ratio of two elements of~$\Bm C$ converges to the ratio of their corresponding elements in~$\CC_{\!\!\Bm{--p}}$ when $ε→0$.  More precisely, for each $i,j,k,l$,
$$
\lim_{ε→0} {C_{i,j}\over C_{k,l}} = \lim_{ε→0} {ε^{-p}\,({\cal C}_{\!-p})_{i,j} + ε^{-p+1}\,({\cal C}_{\!-p+1})_{i,j} + \cdots \over  ε^{-p}\,({\cal C}_{\!-p})_{k,l} + ε^{-p+1}\,({\cal C}_{\!-p+1})_{k,l} + \cdots} = {({\cal C}_{\!-p})_{i,j}\over ({\cal C}_{\!-p})_{k,l}}.~\QED
$$
\postskip

The limiting vector~$\Bm Q$ when $ε→0$ clearly depends on both the data sites $X$ and the data values~$F$ from which it has been computed. The dependence on~$F$ is in the coefficients~$f_j$ of the expressions~\ref[EkWk] for $E_k$ and~$W_k$; the dependence on~$X$ is in the ratios of elements of~$\CC_{\!\!\Bm{--p}}$ that appear in the same expressions when~$ε→0$.

Instead, in the opposite case, that is when $ε→∞$, the vector~$\Bm Q\.$ loses its dependence on~$X$.  In fact, under the usual assumption~\ref[convention] that $\phi(0)=1$ and~$\lim_{r→∞}\phi(r) = 0$,  the interpolation matrix~$\Bm A$ converges to the identity matrix  when $ε→∞$, and this implies that $\norm{s}_\phi^2→ \Bm f^{\,\.T\!}\Bm f$ and $\norm{s^{(k)\!}}_\phi^2→ (\,\Bm f^{\,\.(k)\!})^{T\!}\Bm f^{\,\.(k)}\!$, and finally that $\smash{Q_k→ f_k^{\.2}/(\,\Bm f^{\,\.T\!}\Bm f\,)}$ (here the vectors $\.\Bm f$ and~$\.\smash{\Bm f^{\,\.(k)\!}\!}$ are defined as in Theorem~\ref[rippatheo]).
The case when $ε$~is big, therefore, doesn't seem to be very interesting, because in that case $\Bm Q$ does not store any valuable information about the data sites~$X$.  We also notice that, in the limiting case~$ε→∞$, the vector~$\Bm Q$ has the property that $\norm{\Bm Q}_1 = 1$, independently of the data values~$F$, and the classifier of Definition~\ref[classifierdef] becomes useless.


Although for a fixed value of~$ε$ the vector~$\Bm Q$ is not invariant under a translation applied to~$F$---as already briefly said---fortunately the translational invariance is achieved in the flat limit.\fnote{The limiting case when~$ε→ 0$ is usually called {\,\em flat limit}, because the basis functions generated by~$\phi_ε$ become increasingly flat as~$ε→ 0$.}

\label[translprop]
\preskip
\property If $G = F+η$ for some~$η\in\R$, then
$$
\lim_{ε→ 0} \Bm Q_{\.G\!} = \lim_{ε→ 0} \Bm Q_{\.F}
$$
\proof
Assume by now that $F$ is not constant (meaning that $f_i\neq f_j$ for at least one pair of indices~$i,j$), so that in particular $s_F$ and~$s_G$ are different from the zero function, and the elements $(Q_F)_k$ and~$(Q_G)_k$ are defined as
$$
(Q_F)_k = {(U_F)_k\over\norm{s_F}_\phi^2}
        \buildrel\ref[incrementformula]\over = {(f_k-s_F^{(k)\!}(x_k))^2\over \norm{s_F}_\phi^2\,P_{X^{(k)\!}}^2(x_k)},\qquad
(Q_G)_k = {(U_G)_k\over\norm{s_G}_\phi^2}
        \buildrel\ref[incrementformula]\over = {(g_k-s_G^{(k)\!}(x_k))^2\over \norm{s_G}_\phi^2\,P_{X^{(k)\!}}^2(x_k)}.
$$

The first thing to notice is that both expressions share the term~$P_{X^{(k)\!}}^2(x_k)$, because we are keeping the data sites~$X$ fixed and are changing only the data values from $F$ to~$G$.  Looking at the numerator, we have the elements of the cross validation vectors~$\Bm E_{F\!}$ and $\Bm E_G$.  Using expression~\ref[EkWk], they can be written as
$$
(f_k-s_F^{(k)\!}(x_k))^2 = (E_F)_k = \sum_{j=1}^N f_j\.{C_{k,j}\over C_{k,k}},\qquad
(g_k-s_G^{(k)\!}(x_k))^2 = (E_G)_k = \sum_{j=1}^N g_j\.{C_{k,j}\over C_{k,k}}.
$$
As proved in Property~\ref[flatlimit], when $ε→0$ these two quantities converge,
$$
(f_k-s_F^{(k)\!}(x_k))^2→ \sum_{j=1}^N f_j\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}},\qquad
(g_k-s_G^{(k)\!}(x_k))^2→ \sum_{j=1}^N g_j\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}},
$$
where $\CC_{\!\!\Bm{--p}}$ is the first coefficient matrix in the Laurent series expansion of~$\Bm C$ at~$ε=0$.  The limiting value of~$(E_G)_k$ can be written as
$$
\eqalign{
\sum_{j=1}^N g_j\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}}
&= \sum_{j=1}^N f_j\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}} + \sum_{j=1}^N η\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}} \cr
&=\sum_{j=1}^N f_j\.{(\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}} + η\.{\sum_{j=1}^N (\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}}
}
$$
So, the only difference between the limiting values of the numerators of $(Q_F)_k$ and~$(Q_G)_k$ is the term
$$
 η\.{\sum_{j=1}^N (\Cal C_{-p})_{k,j}\over (\Cal C_{-p})_{k,k}} \eqmark[differenceterm]
$$
We'll come back to it later.

The other difference in the expressions for $(Q_F)_k$ and $(Q_G)_k$ is in the terms $\norm{s_F}_\phi^2$ and~$\norm{s_G}_\phi^2$ at the denominators.  Their expressions are
$$
\norm{s_F}_\phi^2 = \Bm f^{\,\.T\!\!}\Bm C\, \Bm f\qquad\hbox{and}\qquad\norm{s_G}_\phi^2 = (\,\Bm f +η\.\Bm 1)^{T\!}\Bm C\,(\,\Bm f +η\.\Bm 1), 
$$
with $\Bm f \coloneq (\range f_N/)^{T\!}$ and~$\Bm 1\!\.\coloneq(1,1,\dots 1)^{T\!}.$
By using the Laurent series expansion~\ref[laurentC] of~$\Bm C$, in the flat limit their quotient becomes
$$
\eqalign{
\lim_{ε→ 0}{\norm{s_F}_\phi^2\over \norm{s_G}_\phi^2} &=
\lim_{ε→ 0}{\Bm f^{\,\.T\!\!}\.\Bm C\, \Bm f\over (\,\Bm f +η\.\Bm 1)^{T\!}\Bm C\,(\,\Bm f +η\.\Bm 1)} =
\lim_{ε→ 0}{\Bm f^{\,\.T\!}\CC_{\!\!\Bm{--p}}\, \Bm f\over (\,\Bm f +η\.\Bm 1)^{T}\CC_{\!\!\Bm{--p}\!}\,(\,\Bm f +η\.\Bm 1)} \cr
&= \lim_{ε→ 0}{\Bm f^{\,\.T\!}\CC_{\!\!\Bm{--p}}\, \Bm f\over
\Bm f^{\,\.T\!}\CC_{\!\!\Bm{--p}}\, \Bm f + η\.\Bm 1^{\!T\!}\,\CC_{\!\!\Bm{--p}}\,\.\Bm f + η\,\Bm f^{\,\.T\!}\CC_{\!\!\Bm{--p}} \.\Bm 1 + η^2\.\Bm 1^{\!T\!}\.\CC_{\!\!\Bm{--p}}\.\Bm 1}
}
$$

If we show that~$\CC_{\!\!\Bm{--p}} \.\Bm 1 = \Bm 1^{\!T\!}\.\CC_{\!\!\Bm{--p}\!} = \Bm 0$, then this limit is~$1$ and, moreover, expression~\ref[differenceterm] assumes the value~$0$. Then it follows that~$\lim_{ε→ 0} \Bm Q_{\.G\!} = \lim_{ε→ 0} \Bm Q_{\.F}$, as we wanted to show.  The fact that the sum of each row and of each column of the matrix~$\CC_{\!\!\Bm{--p}}$ is zero can be shown by expanding $\Bm A$ and~$\Bm C$ respectively into the power series~\ref[powerA] and the Laurent series~\ref[laurentC]. In fact, the multiplication of $\Bm A$ by~$\Bm C$ yields
$$
\displaylines{
\Bm{AC}= \Bm I \cr
(\CA_{\Bm 0} + \CA_{\Bm 1}\,ε + \CA_{\Bm 2}\,ε^2+\cdots)(\CC_{\!\!\Bm{--p}}\,ε^{-p} + \CC_{\!\!\Bm{--p+}\Bm 1}\,ε^{-p+1} + \CC_{\!\!\Bm{--p+}\Bm 2}\,ε^{-p+2} + \cdots) = \Bm I\cr
(\CA_{\Bm 0}\CC_{\!\!\Bm{--p}})\,ε^{-p} + (\CA_{\Bm 0} \CC_{\!\!\Bm{--p+}\Bm 1} + \CA_{\Bm 1}\CC_{\!\!\Bm{--p}})\,ε^{-p+1} + \cdots = \Bm I. 
}
$$
In this equation, if we multiply both sides by~$ε^{-p\!}$ and then take the limit~$ε→0$, we obtain
$$
\CA_{\Bm 0}\CC_{\!\!\Bm{--p}}= \Bm 0,
$$
where on the right side there is the zero matrix. On the left side, the matrix~$\CA_{\Bm 0}$ is the matrix with all entries equal to~$\phi(0)$.  It is then possible to rewrite the equation as
$$
\phi(0)\,\Bm 1\,\Bm 1^{\!T}\CC_{\!\!\Bm{--p}} = \Bm 0.
$$
Clearly, since $\phi(0)\neq 0$, this relation can hold only if~$\Bm 1^{\!T}\CC_{\!\!\Bm{--p}}$ is the zero vector. The fact that also $\CC_{\!\!\Bm{--p}}\,\Bm 1$ has to be the zero vector is due to the symmetry of the matrix~$\Bm C$ and hence of the coefficient matrix~$\CC_{\!\!\Bm{--p}}$ of its Laurent series.

The proof has not been completed yet, because it remains to see what happens when $F$ is constant (meaning that $f_i=f_j$ for each pair of indices~$i,j$), in which case also $G$ is constant. If $F$ (or~$G$) is constantly equal to~$0$, then $\Bm Q_{\.F}$ (or~$\Bm Q_{\.G}$) is the zero vector by its definition (Definition~\ref[Qdef]). Otherwise, when the constant is different from~$0$, the vector $\Bm Q_{\.F}$ (or~$\Bm Q_{\.G}$)  is again the zero vector because, for each~$k$, the numerator~$(E_F)_k$ of the element~$(Q_F)_k$ (or the numerator~$(E_G)_k$ of the element~$(Q_G)_k$) when~$ε→0$ becomes just like expression~\ref[differenceterm], which evaluates to~$0$.~\QED
\postskip


Property~\ref[scaleprop] and~Property~\ref[translprop] together say that the vector~$\Bm Q\.$ in the flat limit is invariant under a bijective linear transformation of the set~$F$ of function values.
This kind of transformation is nothing more than the $1$-dimensional case of a {\em similarity}.  In general, in dimension~$N$, a similarity is defined as a transformation~${\cal T}:\R^{N\!}→\R^{N\!}$ that can be written as
$$
\Cal T(x) = \rho\, \Cal O(x) + t,\quad x\in\R^N\!\!\!,
$$
with $\rho$ being a positive real number, $\cal O$ an orthogonal\fnote{A transformation is {\em orthogonal} if it can be represented by an orthogonal matrix, that is a matrix whose inverse equals its transpose.  An orthogonal transformation is a rotation, a reflection, or a combination of them.} transformation, and~$t$ a point of~$\R^N\!\!\!$.\,\,   A similarity can be viewed simply as a transformation that preserves the {\em shape} of the object (in our case, of the set of points) to which it is applied.  

% It represent a transformation that doesn't change the shape...

By now, in our analysis, the set~$X$ of data sites has always been kept fixed, and only changes of~$F$ have been considered.  When $X$ gets transformed into
$$
Y = \{\range y_N/\} 
  = \{\Cal T(x_1),\.\Cal T(x_2),\, \dots,\,\Cal T(x_N)\}
  \eqcolon  \Cal T(X),
$$
by means of a similarity~$\Cal T\!$, something similar happens---$\Bm Q$ doesn't change in the flat limit.  We state this fact separated into two properties, considering first a uniform dilation ($\Cal O = \hbox{\EBGaramond\rm Identity}$, $t = 0\in\R^N$) and then a Euclidean transformation ($\.\rho = 1$).  Here the set~$F$ of function values is fixed, so the subscript we'll be used to mark the dependence on the data sites: for instance, $\Bm Q_{X\!}$ will denote the vector~$\Bm Q$ associated to~$(X, F)$, and $\Bm Q_{Y\!}$ will stand for the vector~$\Bm Q$ associated to the transformed data~$(Y, F)$.

\preskip
\property
If $Y = \rho X$ for some real number~$\rho> 0$, then
$$
\lim_{ε→0}\Bm Q_{Y} = \lim_{ε→0}\Bm Q_{X}.
$$ 

\proof
The vector~$\Bm Q_{X\!}$ depends on~$X$ only through the interpolation matrix~$\Bm A_{X}$, which has elements
$$
(A_X)_{i,j} = \phi_ε(\norm{x_i - x_j}) = \phi(ε\.\norm{x_i - x_j}), \quad i,j\in\{1,2,\dots,N\}.
$$
If $X$ becomes~$Y = \rho X$, then each element~$(A_X)_{i,j}$ becomes
$$
(A_Y)_{i,j} = \phi(ε\.\norm{\.\rho\.x_i - r\.x_j}) = \phi(\.\rho\.ε\.\norm{x_i - x_j}).
$$
The constant~$\rho$ uniformly multiplies every occurrence of the parameter~$ε$ in the transformed matrix~$\Bm A_Y$, therefore the change made by~$\rho$ is irrelevant when the limit~$ε→0$ is considered.~\QED
\postskip



\label[shapefig]
\topinsert
\kern-.9cm
\bgroup
\typoscale[900/900]
\picw=1.2\hsize
\line{\hss\inkinspic{shape.pdf}\hbox{\kern1.5em}\hss}
\egroup
\kern-6ex
\caption/f
Three similar sets of data~$(X, F)$, with $X\subset\R$, meaning that they can be transformed one into another by applying a similarity map both on the data sites and the data values.  Intuitively they all have the same regularity, whatever this means.  Indeed, we showed that in the flat limit the regularity vector~$\Bm Q$ is the same for all of them.
\bigskip
\endinsert


Actually, as it is clear from the proof, the scaling parameter~$\rho$ could also have been taken negative.  But the change of sign (the symmetry) is already included in the next result.

\preskip
\property If $Y=\Cal O(X) + t$ for some orthogonal transformation~$\Cal O$ and some point~$t\in\R^N\!\!\!$,\,\, then $\Bm Q_{Y\!} = \Bm Q_{X}$.
\proof
This simply follows from the fact that the interpolation process made with a radial positive definite function is invariant under a Euclidean transformation applied to the data sites.~\QED
\postskip



Summing up, we saw that when $ε→0$ the regularity vector~$\Bm Q$ converges to a limit vector which is invariant under a shape preserving transformation applied to~$X$ or to~$F$ or both.  This fact is very important if we wish to employ~$\Bm Q$ to determine the regularity of some data~$(X, F)$---the regularity of two sets of data that have the same shape should be the same.  Figure~\ref[shapefig] may make this claim apparent.  Since the interesting behaviour of~$\Bm Q$ is achieved in the flat limit, we must find a way to numerically compute a good approximation of $\lim_{ε\to0}\Bm Q$.
