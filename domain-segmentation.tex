

In this section we present the first phase of a two-phase algorithm that aims to recover discontinuous functions from scattered data; the second phase will be disclosed in the following section.  This algorithm is mainly intended to demonstrate the applicability of the regularity classifier described in Chapter~\ref[classificationchap], and must not be viewed as a definitive solution to the problem. Anyhow, it seems to produce satisfactory results. The proposed method greatly takes inspiration from the kernel-based adaptive approximation algorithm outlined by Lenarduzzi and Shaback~\cite[lenarduzzi-shaback_2017] (henceforth simply referred to as L-S). We restrict our attention to the bidimensional case---even if the algorithm allows for a generalisation in arbitrary dimension---because in higher dimensions the required computational cost quickly becomes prohibitive.

The setting of the problem here is similar to that of L-S. Assume that a set~$X=\{\range x_N/\}\subset\Omega\subseteq ℝ^2$ of scattered data sites is given together with an associated set~$F=\{\range f_N/\}$ of function values, sampled from an unknown function~$f$.  Assume to know  that $f$ is continuous on certain non-overlapping subdomains~$\Omega_i\subseteq\Omega$, but not globally.  The goal is to recover the function~$f$ on the whole domain~$\Omega=\bigcup_{i}\Omega_i$, starting from its sampled data~$(X, F)$. There will be no a priori assumptions on the form and placement of the singularities.



If a rough direct interpolation is attempted, without taking into account the discontinuities at the boundaries of the subdomains, then the reproduction quality  could be suboptimal--- the Gibbs phenomenon may appear along the boundaries (Fornberg and Flyer~\cite[fornberg-flyer_2012], Jae-Hun~\cite[jae_hun_2007]), and the discontinuity curves won't be outlined well.
One strategy to successfully recover such a discontinuous function is to first identify the different subdomains, or equivalently their boundaries, and then use this acquired knowledge to perform interpolation on each subdomain separately.  The main principle used here is that ``The approximation properties should determine the domains and their boundaries, not the other way round'' (L-S). First we determine the domains by inspecting the local regularity of the function, and then their boundaries as a consequence.

To implement locality, in L-S a $k$-d tree data structure is employed (Bentley~\cite[bentley_1975]). It is used as a cheap computational method to query for each point~$x\in\Omega$ its $n$ nearest neighbours from~$X$. Here, instead, we prefer to construct the Delaunay triangulation of the set~$X$ (see Aurenhammer, Kein and Lee~\cite[aurenhammer-klein-lee_2013]), because it provides information about the proximity of the points in a more structured way.  From the computational point of view, since we are dealing with an application in dimension~$2$, the difference between the two approaches is not substantial---they both have an average {\Red time complexity}\mnote{\Red\typoscale[700/700]\url{https://en.wikipedia.org/wiki/Time_complexity}} of~$O(N\log N)$.

As centres for our local analysis we use the circumcenters~$C$ of the triangles in the Delaunay triangulation, differently from what is done in L-S, where the centres are the points of~$X$.  This is convenient, because in the Delaunay triangulation a circle circumscribing any triangle does not contain points of~$X$ in its interior, and consequently the set of the $n$ nearest points to a given circumcenter is guaranteed to contain all the vertices of its triangle, if $n\geq3$.  The number~$n$ of local points from~$X$ to consider is kept fixed, independently of the considered circumcenter.  For a given circumcenter the set of its $n$ nearest points from~$X$ may be retrieved by taking advantage of the triangulation (Connor and Kumar~\cite[connor-kumar_2010]), or simply by resorting to the $k$-d tree structure.

We fix also a radial basis function~$\phi$ and a global shape parameter~$ε>0$. Since the goal is to detect discontinuities,  $\phi$ will be either the Wendland's or Matérn's $\Cal C^0$ function---anyhow,  basis functions with greater regularity can be used too, if higher order faults are to be detected.  Furthermore, we choose a tolerance~$\tau$ to build the regularity classifier~$\Cal R$, as in Definition~\ref[classifierdef].  The most important fact in this algorithm is that~{\em $\tau$ is chosen independently of the particular sampled function values~$F$}.  It must be experimentally tuned, but once tuned it can be used for any given data.


First of all, the algorithm starts by picking up one single circumcenter~$c\in C$.  It can be either the first circumcenter of~$C$ (assuming that we have an order on the set~$C$), or one randomly chosen circumcenter. Then, the set~$X_C$ of its $n$ nearest neighbours from~$X$ is taken.
Before interpolation, $ε$ is rescaled to better fit the local fill distance of the interpolating domain. 
