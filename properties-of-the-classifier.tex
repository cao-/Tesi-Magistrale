\def\CA{\mathbox{$\boldmath\cal A$}}
\def\CC{\mathbox{$\boldmath\cal C$}}


\Red
First basic properties, like 0<Q<N, Q=1-Ns/Nsk, ...


Each quantity will be dependent on $ε$, but we don't explicitly denote the dependence, so $s$ is to be intended as...

\preskip
\property Invariance under $F\mapsto\gamma F$
\postskip

\Black

\preskip
\property $\Bm Q$ converges as $ε\to 0$. 
\proof
Notice that $\norm s_\phi = 0$ either for all~$ε$ or for none. When it is zero, $\Bm Q$~is constantly equal to the zero vector, and its convergence is proved.  If instead $\norm s_\phi\neq 0$, then the elements $Q_k$ of the vector~$\Bm Q$ are defined as~$Q_k = U_k/\smash{\norm s_\phi^2}$. By expanding the scalar product in the denominator and recalling the definition of the cross-validation vector~$\Bm E$, we can split~$Q_k$ into two parts, namely
$$
Q_k = {U_k\over \norm s_\phi^2} = {U_k \over \Bm α^T \Bm f} = {E_k {α_k \over \Bm α^T \Bm f}} \eqcolon E_k\,W_k,
$$
where, as usual, $\Bm α$~is the vector of coefficients of~$s$ with respect to the basis of kernel translates. Both terms $E_k$ and~$W_k$ can be written in terms ratios of pairs of elements of the inverse~$\Bm C$ of the interpolation matrix~$\Bm A=\Bm A_{\phi, X}$.  In fact,
$$
\eqalign{
  &E_k = {α_k \over C_{k,k}} = {(\Bm C\. \Bm f\,)_k \over C_{k,k}} =  {\sum_{j=1}^N C_{k,j}\, f_j \over C_{k,k}} = \sum_{j = 1}^N f_j\,\frame{$\displaystyle \hbyw{1.8ex}{0em} C_{k,j} \over \displaystyle C_{k,k}$}\,,\cr
 &\eqalign{W_k = {α_k \over \Bm α^T \Bm f} = {α_k \over \Bm f^{\,\.T}\! \Bm C\, \Bm f} &= {\sum_{j=1}^N C_{k,j}\,f_j \over \sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l}} = \sum_{j=1}^N\. f_j\. {C_{k,j} \over \sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l}}\cr
    &= \sum_{\rlap{$\matrix{j = 1\,\,\cr C_{k,j}\neq 0}$}}^N\, \Biggl({\sum_{i=1}^N \sum_{l=1}^N f_i\, f_l\,C_{i,l} \over C_{k,j}}\Biggr)^{-1}\!\! = \sum_{\rlap{$\matrix{j = 1\,\,\cr C_{k,j}\neq 0}$}}^N f_j\,\Bigl(\sum_{i=1}^N\sum_{l=1}^N f_i\, f_l\, \frame{$\displaystyle \hbyw{1.8ex}{0em} C_{i,l} \over \displaystyle C_{k,j}$}\,\Bigr)^{\rlap{$\scriptstyle-1$}}.
  } 
}
$$
The dependence on~$ε$ of the quantities $E_k$ and~$W_k$ lies  only in the highlighted ratios.  If we show that those terms converge as~$ε\to 0$, then we are done.

We can fix a radius~$r\geq 0$ and view $\phi_ε(r)$ as a function of~$ε$ rather than~$r$, and consider its power series expansion at the point~$ε=0$,
$$
\phi_ε(r) = a_{0\!}(r) + a_{1}\!(r)\,ε + a_{2\!}(r)\,ε^2 + a_{3\!}(r)\,ε^3 + \cdots,
$$
whose coefficients~$a_{n\!}(r)$ are explicit functions of~$r$.
With $\Bm R$ being the distance matrix associated to~$X$, that is the $N\times N$~matrix of elements~$R_{i,j}\coloneq \norm{x_i - x_j}$, define for each~$n$ the matrix~$\CA_{\Bm n}$ by applying the function $a_n$ to each element of~$\Bm R$, explicitly
$$
(\Cal A_{n})_{i,j}\coloneq a_n(R_{i,j}) = a_n(\norm{x_i - x_j}).
$$
Under these definitions, the $ε$-dependent matrix~$\Bm A$ has at the point~$ε=0$ the power series expansion
$$
 \Bm{A} = \CA_{\Bm 0} + \CA_{\Bm 1}\,ε + \CA_{\Bm 2}\,ε^2 + \CA_{\Bm 3}\,ε^3 + \cdots.
$$



 The inverse~$\Bm C$ of the matrix~$\Bm A$ exists for all~$ε$ and has elements that are rational functions of the elements of~$\Bm A$.  This means that $\Bm C$ has a Laurent series expansion at~$ε=0$,
$$
\Bm C =  \CC_{\!\!\Bm{--p}}\,ε^{-p} + \CC_{\!\!\Bm{--p+}\Bm 1}\,ε^{-p+1} + \CC_{\!\!\Bm{--p+}\Bm 2}\,ε^{-p+2} + \CC_{\!\!\Bm{--p+}\Bm 3}\,ε^{-p+3} + \cdots,
$$
where $p$ is the order of singularity at $ε=0$ of the matrix~$\Bm C$ (Refer to Langenhop~\cite[langenhop_1971] for a general treatment of this topic, and also to Gonzales-Rodriguez, Moscoso and Kindelan~\cite[gonzales_rodriguez-moscoso-kindelan_2015] for a specific application in the context of radial basis functions).
This means that near~$ε=0$ the matrix~$\Bm C$ is just like the first term~$\CC_{\!\!\Bm{--p}}\,ε^{-p}$ of its Laurent series, therefore the ratio of two elements of~$\Bm C$ converges to the ratio of their corresponding elements in~$\CC_{\!\!\Bm{--p}}$ when $ε\to 0$.  More precisely, for each $i,j,k,l$,
$$
\lim_{ε\to 0} {C_{i,j}\over C_{k,l}} = \lim_{ε\to 0} {ε^{-p}\,({\cal C}_{\!-p})_{i,j} + ε^{-p+1}\,({\cal C}_{\!-p+1})_{i,j} + \cdots \over  ε^{-p}\,({\cal C}_{\!-p})_{k,l} + ε^{-p+1}\,({\cal C}_{\!-p+1})_{k,l} + \cdots} = {({\cal C}_{\!-p})_{i,j}\over ({\cal C}_{\!-p})_{k,l}}.~\QED
$$
\postskip

\Red

\preskip
\property Invariance under $F\mapsto F+c$ in the flat limit
\postskip
Also, invariance under modification of $X$.  So, linear invariance complexively.

Ho to deal numerically with the flat limit.


These are the properties of Q that are carried over to R.  Now come the properties for R.

\preskip
\property
$\sum Q \to 0$ if $f\in \Cal N_\phi$.  Otherwise...
\postskip

\Black
