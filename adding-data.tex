

Up to now we have always considered the problem of recovering an unknown function from a fixed set of information about it, namely the values that the function assumes on a given set of points.  But never we considered what happens to the interpolant when our knowledge on the sampled function gradually increases, that is when we have access to more and more sampled data.  It is reasonable to expect that the reconstructed function becomes increasingly more similar to the original function. This is the topic of concern of this section.


Let $X$ and $Y$ be two sets of data sites such that~$X\subset Y\!$.  Given a function~$f$ in the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ$, we can compute both of the interpolants $s_{X,f}\in\Cal S_{Φ, X}$ and~$s_{Y,f}\in\Cal S_{Φ, Y}$.  It is easy to see that, as expected,  $s_{Y,f}$ is no worse than~$s_{X,f}$ in approximating~$f$, meaning that 
$$
\norm{\.f - s_{Y,f}}_Φ \leq \norm{\.f - s_{X,f}}_Φ.
$$
This is an immediate consequence of the inclusion~$\Cal S_{Φ, X}\subset\Cal S_{Φ, Y}$ and the property~\ref[mindist] of minimum distance satisfied by~$s_{Y,f}$.  If we then combine the Pythagorean identities~\ref[pythag] satisfied by $s_{X,f}$ and~$s_{Y,f}$ with the above inequality we further obtain that
$$
\norm{s_{X, f}}_Φ\leq\norm{s_{Y,f}}_Φ\leq\norm{\.f}_Φ.\eqmark[normbound]
$$

Assuming that $\{Y_n\}_{n\in\N}$ is a growing sequence of data sites, in the sense that
$$
Y_1\subset Y_2\subset Y_3\subset \cdots,
$$
then clearly, by monotonicity, the sequence~$\{\norm{s_{Y_n,f}}_Φ\}_{n\in\N}$ of the native space norms of the associated interpolants of~$f$ is a convergent sequence, whose limit is a number less than or equal to~$\norm{\.f}_Φ$. Whether or not this limit actually equals~$\norm{\. f}_Φ$ may depend not only on the number of data sites but also on their distribution among the domain of~$f$.

At least for the pointwise limit there are some specific results, which we have already implicitly stated in Section~\ref[errorsec], talking about error estimates.  If the domain~$Ω$ of~$f$ satisfies an interior cone condition and the sequence~$\{Y_n\}_{n\in\N}$ of data sites grows in a way such that the fill distances~$\{h_{Y_n}\}_{n\in\N}$ converge to zero, then the sequence of interpolants~$\{s_{Y_n,f}\}_{n\in\N}$ converges uniformly to~$f$, i.e.,
$$
\norm{\.f - s_{Y_n,f}}_\infty \coloneq \sup_{x\in Ω} |f(x)-s_{Y_n,f}(x)\!|\to 0\qquad\hbox{when $\,n\to\infty$},
$$
because of theorem~\ref[errorestimate] and the uniform bound on the power function given in~\ref[powbound]. 





For the increase of the  norm of the interpolant when the set of data sites grows there is actually an explicit formula, as shown by Shaback and~Werner~\cite[shaback_2006].  Specifically, in the simple case where $Y=X\cup\{x\}$, that is when just one point is added to the data sites, the following result holds.

\preskip
\theorem
Let~$f$ be a function in the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\Omega\times\Omega\to\R$.  If $X\subset\Omega$ is a set of locations and~$x\in\Omega$ is a point {\em not}  belonging to~$X$, then the norm of the interpolant~$s_{X\cup\{x\},f}\in\Cal S_{Φ, X\cup\{x\}}$ of~$f$ on the extended set of locations~$X\cup\{x\}$ is related to the norm of the interpolant~$s_{X, f}\in\Cal S_{Φ, X}$ of~$f$ on~$X$ by the expression
$$
\norm{s_{X\cup\{x\}, f}}_Φ^2 = \norm{s_{X, f}}_Φ^2 + {(f(x) - s_{X, f}(x))^2\over P^2_X(x)},
$$
where $P_X$ is the power function associated to~$X$.

\proof
Let $X=\{\range x_N/\}$, and define the  matrix~$\Bm A$ and the vectors~$\Bm u(x), \Bm t(x)$ as in~\ref[uandt].
The interpolants $s_{X, f}$ and~$s_{X\cup\{x\},f}$ can be expressed as linear combinations of the standard basis functions of $\Cal S_{Φ, X}$ and~$\Cal S_{Φ, X\cup\{x\}}$ respectively. Explicitly,
$$
s_{X,f} = \sum_{k = 1}^Nα_k Φ(\cdot, x_k),  \qquad
s_{X\cup\{x\}, f} = \sum_{k=1}^Nβ_k Φ(\cdot, x_k) + γ\, Φ(\cdot, x),
$$
for some coefficients~$\{α_k\}_{k\in\{1,2,\dots,N\}}$, $\{\.β_k\}_{k\in\{1,2,\dots,N\}}$ and~$γ$.
If we define the vectors 
$$
\Bm α \coloneq (\rangeα_N/)^T \quad\hbox{and}\quad \Bm β \coloneq (\rangeβ_N/)^T,
$$
 then the evaluations of the two~interpolants at the points $\{x_j\}_{j\in\{1,2,\dots,N\}}$ of~$X$ have the expressions
$$
s_{X,f}(x_j) =\Bm t(x_j)^T\!\Bmα\quad\hbox{and}\quad s_{X\cup\{x\},f}(x_j)  = \Bm t(x_j)^T\!\Bmβ + γ\,Φ(x_j,x),
$$
which can be collected in a compact way as
$$
\pmatrix{
s_{X,f}(x_1) \cr
s_{X,f}(x_2) \cr
\vdots 	    \cr
s_{X,f}(x_N)
} = \Bm A \Bm α\., \quad\hbox{and}\quad
\pmatrix{
s_{X\cup\{x\},f}(x_1) \cr
s_{X\cup\{x\},f}(x_2) \cr
\vdots 	    \cr
s_{X\cup\{x\},f}(x_N)
} = \Bm A \Bm β + γ\Bm t(x),
$$
simply by noticing that~$\Bm t(x_j)^T$ is the $j$-th row of the matrix~$\Bm A$.  Since by definition the interpolants $s_{X, f}$ and~$s_{X\cup\{x\}, f}$ assume the same value at each point of~$X$,  the relation
$$
\Bm A \Bm α = \Bm A \Bm β + γ \Bm t(x)
$$
must hold.  After having multiplied both sides by the inverse of~$\Bm A$ and having rearranged its terms, this equation gives an expression for the vector of coefficients~$\Bm β$, that is
$$
\eqalign{
\Bm β &= \Bm α - γ \Bm A^{-1} \Bm t(x) \cr
		  &= \Bm α - γ\. \Bm u(x). 
} \eqmark[proof*]
$$

We now compute the squares of the native space norms of $s_{X, f}$ and~$s_{X\cup\{x\}, f}$ by expanding the scalar product as it is defined in~\ref[form].  For~$s_{X, f}$ we get
$$
\norm{s_{X,f}}_Φ^2 = \bnorm{\sum_{j=1}^N α_j Φ(\cdot, x_j)}_Φ^2 = \sum_{j=1}^N\sum_{k=1}^N α_j α_k Φ(x_j, x_k) =  \Bm α^T\! \Bm A \.\Bm α,
$$
while for~$s_{X\cup\{x\}, f}$ analogous computations yield
$$
\eqalign{
\norm{s_{X\cup\{x\}, f}}_Φ^2 &= \bnorm{\sum_{j = 1}^N β_j Φ(\cdot, x_j)}_Φ^2 + 2\,\bform{\sum_{j=1}^N β_j Φ(\cdot, x_j)}{γ\, Φ(\cdot, x)}_Φ + \norm{γ\, Φ(\cdot, x)}_Φ^2 \cr
&= \sum_{j=1}^N\sum_{k=1}^N β_j β_k Φ(x_j, x_k) + 2γ\sum_{j = 1}^N β_j Φ(x_j, x) + γ^2 Φ(x,x) \cr
&= \Bm β^T\! \Bm A \.\Bm β + 2 γ\. \Bm t(x)^T \Bm β + γ^2 Φ(x,x).
}
$$
Then, the substitution~\ref[proof*] for~$\Bm β$ allows us to express the square of the norm of~$s_{X\cup\{x\}, f}$ as
$$
\eqalign{
\norm{s_{X\cup\{x\}, f}}_Φ^2 &= (\Bm α - γ\Bm u(x))^T\Bm A\, (\Bm α - γ \Bm u(x)) + 2γ\. \Bm t(x)^T\! (\Bm α - γ \Bm u(x)) + γ^2 Φ(x,x) \cr
&=  \Bm α^T\!\Bm A\.\Bm α +γ^2 \bigl[Φ(x,x) - 2\Bm t(x)^T\Bm u(x) + \Bm u(x)^T\!\Bm A\. \Bm u(x) \bigr] \cr
&= \norm{s_{X,f}}_Φ^2 + γ^2 P^2_X(x),
}\eqmark[proof**]
$$
where in the last step we recognised the power function from expression~\ref[pow1].

It is finally possible to find an expression for~$γ$ by computing the difference of the interpolants at the addded point~$x$.  In fact,
$$
\eqalign{
s_{X\cup\{x\}, f}(x) - s_{X, f}(x) &= \Bm t(x)^T\Bm β + γ\, Φ(x,x) - \Bm t(x)^T\Bmα \cr
&= \Bm t(x)^T(\Bm α - γ\Bm u(x)) + γ\,Φ(x,x) - \Bm t(x)^T\Bmα \cr
&= γ\.\bigl[Φ(x,x) - \Bm t(x)^T\Bm u(x)\bigr] \cr
&= γ P^2_X(x),
}
$$
where this time we used expression~\ref[pow2] to recognise~$P_X(x)$, and hence
$$
γ = {s_{X\cup\{x\}, f}(x) - s_{X, f}(x)\over P^2_X(x)}.
$$
The substitution of this value of~$γ$ into expression~\ref[proof**] concludes the proof.~\QED
\postskip


On the one hand this theorem says that if the interpolant~$s_{X,f}$ already makes a correct prediction at a given point~$x\notin X$, in the sense that $s_{X,f}(x)=f(x)$, then the addition of that point~$x$ to the data sites has no effect on the norm of the interpolant, in fact
$$
\norm{s_{X\cup\{x\},f}}_Φ^2=\norm{s_{X,f}}_Φ^2+ {(f(x) - s_{X, f}(x))^2\over P^2_X(x)}=\norm{s_{X,f}}_Φ^2.
$$
But this is something that we already knew, since in this case the interpolant itself doesn't change, because $s_{X,f}$, just like $s_{X\cup\{x\},f}$, is a function of~$\Cal S_{Φ, X\cup\{x\}}$ which interpolates~$f$ at the data sites~$X\cup\{x\}$, and the solution to the interpolation problem is unique in~$\Cal S_{Φ, X\cup\{x\}}$.

On the other hand, if $s_{X,f}$ predicts at the point~$x$ a value~$s_{X,f}(x)$ different from the correct value~$f(x)$, then the difference between the squares of the norms of $s_{X\cup\{x\},f}$ and~$s_{X,f}$, which by the above theorem can be computed as
$$
\norm{s_{X\cup\{x\},f}}_Φ^2 -\norm{s_{X,f}}_Φ^2= {(f(x) - s_{X, f}(x))^2\over P^2_X(x)},
$$
may be large not only if the prediction is bad, but also if the value of~$P_X$ at the new point~$x$ is small, which happens when $x$~is close to one of the points of~$X$.  Instead, in case $Φ$ is radial and for simplicity taken such that properties~\ref[convention] hold, when $x$ is sufficiently far from all the points of~$X$ the power function has no more influence on the difference between the squares of the native space norms, which in fact has value approximately equal simply to~$(f(x)-s_{X,f}(x))^2$. Refer to figure~\ref[powfig] for the justification of this fact.



Therefore, as pointed out by Lenarduzzi and Shaback~\cite[lenarduzzi-shaback_2017], computing the difference $\norm{s_{X\cup\{x\},f}}_Φ^2 -\norm{s_{X,f}}_Φ^2$ between the squares of the norms of $s_{X\cup\{x\},f}$ and~$s_{X,f}$ is a geometrically correct way of establishing the goodness of the prediction made by $s_{X,f}$ for the value of~$f$ at the point~$x$, since this quantity does not consider only the error~$f(x)-s_{X,f}(x)$, but also takes into account, through the value~$P_X(x)$, the closeness of~$x$ to the data sites~$X$. We expect in fact a much better accuracy on the prediction for~$f(x)$ at a point $x$ close to~$X$ compared to a point $x$ far from~$X$.


If the set~$X$ of data sites is already such that $s_X$ is close to~$f$ in~$\Cal N_Φ$, and hence $\norm{s_{X,f}}_Φ$ is close to~$\norm{\.f}_Φ$, then there is few margin left for a further increase of the norm of the interpolant when a new point~$x$ is added to the data sites, since \ref[normbound]~says that~$\norm{s_{X, f}}_Φ\leq\norm{s_{X\cup\{x\},f}}_Φ\leq\norm{\.f}_Φ.$  This last statement of course holds under the hypothesis that the sampled function~$f$ belongs to the native space of~$Φ$.  If this is not the case, then we do not have a bound on the possible increase of the norm when passing from $s_{X,f}$ to~$s_{X\cup\{x\},f}$.

 A case of special interest is that of a {\em discontinuous} function~$f$, which of course doesn't belong to the native space of a continuous kernel~$Φ$ (see theorem~\ref[cont-theor]). In this case the norm of the interpolant can grow very much if the newly added point $x$ is close to the singularity of~$f$.  The big increment of the norm indicates that a violation of the model~$s_{X,f}$ is occurring, or in other words that the new pair of data~$(x, f(x))$ does not fit well the model~$s_{X,f}$ previously established by the pairs of data~$(X, f(X))$.


% TODO: the norm of the interpolating converges, but not necessarily to the norm of the function.  Example of this.   The location/distribution of the data sites is important.  Results for pointwise convergence.







\comment

Let us now consider a nested infinite sequence~$\{ Y_n\}_{n\in\N}$ of sets of locations,
$$
X=Y_1\subset Y_2 \subset Y_3 \subset \cdots \subset\R^d
$$
and the associated infinite sequence~$\{s_{Y_n, f}\}_{n\in\N}$ of interpolants of a function~$f\in\Cal N_Φ$.

\endcomment


