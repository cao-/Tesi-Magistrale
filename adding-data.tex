

Up to now we have always considered the problem of recovering an unknown function from a fixed set of information about it, namely the values that the function assumes on a given set of points.  But never we considered what happens to the interpolant when our knowledge on the sampled function gradually increases, that is when we have access to more and more sampled data.  It is reasonable to expect that the reconstructed function becomes increasingly more similar to the original function. In this section we are going to discuss this topic; moreover, we will also see how the interpolating error of a function relates  to the size of the sampling and its distribution among the domain of the function.



Let $X$ and $\.Y$ be two sets of data sites such that~$X\subset Y$.  Given a function~$f$ in the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ$, we can compute both of the interpolants $s_{X,f}\in\Cal S_{Φ,\. X}$ and~$s_{Y\!,f}\in\Cal S_{Φ, Y}$.  It is easy to see that, as expected,  $s_{Y\!,f}$ is no worse than~$s_{X,f}$ at approximating~$f$, meaning that 
$$
\norm{\.f - s_{Y\!,f}}_Φ \leq \norm{\.f - s_{X,f}}_Φ.
$$
This is an immediate consequence of the inclusion~$\Cal S_{Φ,\. X}\subset\Cal S_{Φ, Y}$ and the property~\ref[mindist] of minimum distance satisfied by~$s_{Y\!,f}$.  If we then combine the Pythagorean identities~\ref[pythag] satisfied by $s_{X,f}$ and~$s_{Y\!,f}$ with the above inequality we further obtain that
$$
\norm{s_{X, f}}_Φ\leq\norm{s_{Y\!,f}}_Φ\leq\norm{f}_Φ.
$$
%So, when additional information on the sampled function~$f$ is available, not only the interpolant  provides a better approximation, but also its native space norm increases. 

There is actually an explicit expression for the increase in norm of the interpolant, as shown by Shaback and~Werner~\cite[shaback_2006].  Specifically, in the simple case where $Y=X\cup\{x\}$, that is when just one point is added to the data sites, the following result holds.

\preskip
\theorem
Let~$f$ be a function in the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\Omega\times\Omega\to\R$.  If $X\subset\Omega$ is a set of locations and~$x\in\Omega$ is a point {\em not}  belonging to~$X$, then the norm of the interpolant~$s_{X\cup\{x\},f}\in\Cal S_{Φ, X\cup\{x\}}$ of~$f$ on the extended set of locations~$X\cup\{x\}$ is related to the norm of the interpolant~$s_{X, f}\in\Cal S_{Φ, X}$ of~$f$ on~$X$ by the expression
$$
\norm{s_{X\cup\{x\}, f}}_Φ^2 = \norm{s_{X, f}}_Φ^2 + {(f(x) - s_{X, f}(x))^2\over P^2_X(x)},
$$
where $P_X$ is the power function associated to~$X$.

\proof
Let $X=\{\range x_N/\}$, and define the  matrix~$\Bm A$ and the vectors~$\Bm u(x), \Bm b(x)$ as in section~\ref[errorsec].
The interpolants $s_{X, f}$ and~$s_{X\cup\{x\},f}$ can be expressed as linear combinations of the standard basis functions of $\Cal S_{Φ, X}$ and~$\Cal S_{Φ, X\cup\{x\}}$ respectively. Explicitly,
$$
s_{X,f} = \sum_{j = 1}^N\alpha_j Φ(\cdot, x_j),  \qquad
s_{X\cup\{x\}, f} = \sum_{j=1}^N\beta_j Φ(\cdot, x_j) + \gamma\, Φ(\cdot, x),
$$
for some coefficients~$\{\alpha_j\}_{j\in\{1,2,\dots,N\}}$, $\{\.\beta_j\}_{j\in\{1,2,\dots,N\}}$ and~$\gamma$.
If we define the vectors 
$$
\Bm \alpha \coloneq (\range\alpha_N/)^T \quad\hbox{and}\quad \Bm \beta \coloneq (\range\beta_N/)^T,
$$
 then the evaluations of the two~interpolants at the points $\{x_j\}_{j\in\{1,2,\dots,N\}}$ of~$X$ have the expressions
$$
s_{X,f}(x_j) =\Bm b(x_j)^T\!\Bm\alpha\quad\hbox{and}\quad s_{X\cup\{x\},f}(x_j)  = \Bm b(x_j)^T\!\Bm\beta + \gamma\,Φ(x_j,x),
$$
which can be collected in a compact way as
$$
\pmatrix{
s_{X,f}(x_1) \cr
s_{X,f}(x_2) \cr
\vdots 	    \cr
s_{X,f}(x_N)
} = \Bm A \Bm \alpha\., \quad\hbox{and}\quad
\pmatrix{
s_{X\cup\{x\},f}(x_1) \cr
s_{X\cup\{x\},f}(x_2) \cr
\vdots 	    \cr
s_{X\cup\{x\},f}(x_N)
} = \Bm A \Bm \beta + \gamma\Bm b(x),
$$
simply by noticing that~$\Bm b(x_j)^T$ is the $j$-th row of the matrix~$\Bm A$.  Since by definition the interpolants $s_{X, f}$ and~$s_{X\cup\{x\}, f}$ assume the same value at each point of~$X$,  the relation
$$
\Bm A \Bm \alpha = \Bm A \Bm \beta + \gamma \Bm b(x)
$$
must hold.  After having multiplied both sides by the inverse of~$\Bm A$ and having rearranged its terms, this equation gives an expression for the vector of coefficients~$\Bm \beta$, that is
$$
\eqalign{
\Bm \beta &= \Bm \alpha - \gamma \Bm A^{-1} \Bm b(x) \cr
		  &= \Bm \alpha - \gamma\. \Bm u(x). 
} \eqno(\lower.6ex\hbox{*})
$$

We now compute the squares of the native space norms of $s_{X, f}$ and~$s_{X\cup\{x\}, f}$ by expanding the scalar product as it is defined in~\ref[form].  For~$s_{X, f}$ we get
$$
\norm{s_{X,f}}_Φ^2 = \bnorm{\sum_{j=1}^N \alpha_j Φ(\cdot, x_j)}_Φ^2 = \sum_{j=1}^N\sum_{k=1}^N \alpha_j \alpha_k Φ(x_j, x_k) =  \Bm \alpha^T\! \Bm A \.\Bm \alpha,
$$
while for~$s_{X\cup\{x\}, f}$ analogous computations yield
$$
\eqalign{
\norm{s_{X\cup\{x\}, f}}_Φ^2 &= \bnorm{\sum_{j = 1}^N \beta_j Φ(\cdot, x_j)}_Φ^2 + 2\,\bform{\sum_{j=1}^N \beta_j Φ(\cdot, x_j)}{\gamma\, Φ(\cdot, x)}_Φ + \norm{\gamma\, Φ(\cdot, x)}_Φ^2 \cr
&= \sum_{j=1}^N\sum_{k=1}^N \beta_j \beta_k Φ(x_j, x_k) + 2\gamma\sum_{j = 1}^N \beta_j Φ(x_j, x) + \gamma^2 Φ(x,x) \cr
&= \Bm \beta^T\! \Bm A \.\Bm \beta + 2 \gamma\. \Bm b(x)^T \Bm \beta + \gamma^2 Φ(x,x).
}
$$
Then, the substitution~(\lower.6ex\hbox{*}) for~$\Bm \beta$ allows us to express the square of the norm of~$s_{X\cup\{x\}, f}$ as
$$
\eqalign{
\norm{s_{X\cup\{x\}, f}}_Φ^2 &= (\Bm \alpha - \gamma\Bm u(x))^T\Bm A\, (\Bm \alpha - \gamma \Bm u(x)) + 2\gamma\. \Bm b(x)^T\! (\Bm \alpha - \gamma \Bm u(x)) + \gamma^2 Φ(x,x) \cr
&=  \Bm \alpha^T\!\Bm A\.\Bm \alpha +\gamma^2 \bigl[Φ(x,x) - 2\Bm b(x)^T\Bm u(x) + \Bm u(x)^T\!\Bm A\. \Bm u(x) \bigr] \cr
&= \norm{s_{X,f}}_Φ^2 + \gamma^2 P^2_X(x),
}\eqno(\lower.6ex\hbox{*}\lower.6ex\hbox{*})
$$
where in the last step we recognised the power function from expression~\ref[pow1].

It is finally possible to find an expression for~$\gamma$ by computing the difference of the interpolants at the adjoined point~$x$.  In fact,
$$
\eqalign{
s_{X\cup\{x\}, f}(x) - s_{X, f}(x) &= \Bm b(x)^T\Bm \beta + \gamma\, Φ(x,x) - \Bm b(x)^T\Bm\alpha \cr
&= \Bm b(x)^T(\Bm \alpha - \gamma\Bm u(x)) + \gamma\,Φ(x,x) - \Bm b(x)^T\Bm\alpha \cr
&= \gamma\bigl[Φ(x,x) - \Bm b(x)^T\Bm u(x)\bigr] \cr
&= \gamma P^2_X(x),
}
$$
where this time we used expression~\ref[pow2] to recognise~$P_X(x)$, and hence
$$
\gamma = {s_{X\cup\{x\}, f}(x) - s_{X, f}(x)\over P^2_X(x)}.
$$
The substitution of this value of~$\gamma$ into expression~(\lower.6ex\hbox{*}\lower.6ex\hbox{*}) concludes the proof.\hfill\QED
\postskip


% TODO: the norm of the interpolating converges, but not necessarily to the norm of the function.  Example of this.   The location/distribution of the data sites is important.  Results for pointwise convergence.



Starting from inequality~\ref[pow3] it is possible to derive an upper bound for the power function in terms of how well the data sites~$X\subset\Omega$ fill the domain~$\Omega$, which is measured by the {\em fill distance}~$h_X$, defined as
$$
h_X = \sup_{x\in\Omega}\min_{x_j\in X}\norm{x-x_j}.
$$
If $\Omega$ is bounded and satisfies an interior cone condition,\fnote{A domain~$\Omega\in\R^d$ satisfies an {\em interior cone condition} if there exists an angle~$\theta\in(0,\pi/2)$ and a radius~$r>0$ such that for every~$x\in\Omega$ there exists a unit vector~$\xi(x)$ such that the cone
\vskip-\parskip
\vskip-1.5ex
$$
C = \{x+\lambda y:\, y\in\R^d,\, \norm y = 1,\, y^T\xi(x)\geq\cos\theta,\, \lambda\in[0,r]\}
$$
\vskip-\parskip
\vskip-1ex \noindent
is contained in~$\Omega$.} then 
$$
P_X^2(x)\leq F(h_X) \quad \mathbox{for all $x\in\Omega$,}
$$
for some function~$F$ which goes to zero as~$h_X\to 0$ (Wendland~\cite[wendland_2004], chapter~11).  The function~$F$ depends on the kernel of the native space, and can be explicitly computed.  In table~\ref[pow-tab] we report the known bounds on the power function for some of the most used radial kernels.

\label[pow-tab]
\topinsert
\centerline{
\table{lcc}{\crl
                        & $\phi(r)$               & $F(h)$  \crl\tskip2pt
Gaussians               & $e^{-\epsilon r^2}$,\quad $\epsilon>0$        & $e^{-c\,|\!\log h|/h}$,\quad  $c>0$ \cr\tskip2pt
Inverse multiquadrics   & $(1+r)^{-\epsilon}$,\quad $\epsilon>0$ & $e^{-c/h}$,\quad  $c>0$ \cr \tskip2pt
Wendland's functions    & $\phi_{d,k}(r)$           & $h^{2k+1}$ \cr\tskip5pt
\crl
}}
\caption/t Upper bounds on~$P_X^2$ as functions~$F(h)$ of the fill distance~$h$. The functions $F$ are given only up to a constant that does not depend on the data sites.  They express the decay rate of the power function as the fill distance approaches zero.
\endinsert




