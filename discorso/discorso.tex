%\fontfam[MTBaskerville]\typosize[11/13]
\fontfam[lm]
%\adef’{'}
%\adef“{``}
%\adef”{''}

\def\R{\BbbR}

\parindent 0pt
\parskip 3ex plus .3ex minus .3ex


\def\begitems{\_bgroup\_begitems\_parskip=1.5\lineskip \_removelastskip}
\def\enditems{\_enditems\par\vskip-\parskip\_egroup}
\def\_belowliskip{\_penalty -200 \vskip.2\baselineskip}


In questa tesi mi sono occupato del problema della ricostruzione di una funzione discontinua a partire da dati sparsi.

Con {\em dati sparsi} si intende che l'insieme di locazioni in cui la funzione è stata campionata non ha una disposizione strutturata all'interno del dominio, e in generale non si hanno particolari ipotesi su tale disposizione. 
Qui è visualizzato il grafico di una funzione definita sul quadrato unitario e discontinua lungo una curva.    Se si tenta di ricostruire direttamente una tale  funzione senza particolari accorgimenti, si incorre nel fenomeno di Gibbs, per cui la funzione ricostruita presenta forti oscillazioni nei pressi delle curve di discontinuità. 


L'obiettivo del lavoro di tesi consiste nel proporre una tecnica che permetta di ricostruire accuratamente una funzione di due variabili a partire da un insieme di dati sparsi, costituito da un insieme~$X$ di locazioni contenuto in un sottoinsieme~$\Omega$ di $\R^2$ e da un insieme~$F$ di valori assunti dalla funzione in tali locazioni.  Si vuole dunque evitare che si manifesti il fenomeno di Gibbs, e si vuole riprodurre fedelmente la funzione in tutto il dominio e in particolare nei pressi delle sue curve di discontinuità.  L'ipotesi del problema è che la funzione  sia regolare su certi sottoinsiemi~$\Omega_j$ che partizionano~$\Omega$, e che sia discontinua lungo il bordo di ciascun sottoinsieme:  cioè si assume che sia discontinua lungo delle curve in~$\Omega$ e che non abbia ad esempio discontinuità in punti isolati.  Tuttavia non si hanno informazioni sui sottodomini~$\Omega_j$ né sulle curve di discontinuità.


Per ricostruire accuratamente la funzione è necessario innanzitutto  recuperare i sottodomini in cui essa è regolare.  La tecnica che si propone consiste di tre fasi:
\begitems
* Inizialmente si raggruppa l'insieme di locazioni $X$ in sottoinsiemi~$X_j$ tali che ciascun sottoinsieme sia contenuto completamente nel suo corrispettivo sottodominio~$\Omega_j$.  Cioè si ricostruisce una prima versione discreta dei sottodomini~$\Omega_j$.   In questa fase si analizzano localmente i dati tramite un {\em classificatore} di regolarità, costruito utilizzando interpolanti con basi radiali;
*Successivamente si ricostruiscono effettivamente i sottodomini~$\Omega_j$.  In questa seconda fase si utilizza un modello di machine learning chiamato {\em support vector machines}.
* Infine si interpolano i dati su ogni sottodominio separatamente, utilizzando ancora le basi radiali.
\enditems\null


Per poter definire il classificatore di regolarità e quindi spiegare la prima fase del metodo proposto, introduco i concetti di {\em kernel} e {\em spazi nativi} che sono alla base delle tecniche di interpolazione utilizzate nel contesto di dati sparsi.
Un kernel~$K$ definito su un dominio $\Omega$ contenuto in~$\R^d$ è una funzione da $\Omega{\times}\Omega$ a valori in~$\R$ che è simmetrica e definita positiva.  Essa genera uno spazio di Hilbert, chiamato {\em spazio nativo}, in cui $K$ risulta essere {\em riproducente}.
Lo spazio nativo è definito come il completamento  secondo il prodotto scalare qui  indicato dello spazio lineare generato dalle traslate del kernel.

Nello spazio nativo il problema dell'interpolazione di dati con locazioni distinte ha sempre soluzione.  Infatti una funzione che interpola i valori di $f$ sulle locazioni~$X$ si può ottenere tramite combinazione lineare di traslate del kernel centrate in tali locazioni.   I coefficienti di questa combinazione lineare derivano dalla risoluzione di un sistema lineare la cui matrice è definita positiva.
 La norma dell’interpolante  è una quantità effettivamente calcolabile, infatti si ottiene semplicemente considerando il prodotto scalare tra i coefficienti di interpolazione e i valori campionati dalla funzione.





I kernel più utilizzati nel contesto dell'interpolazione di dati sparsi sono i kernel {\em radiali}.  Un kernel si dice radiale se il suo valore in una qualsiasi coppia di punti $x,y$ si ottiene valutando una funzione $\phi$ sulla norma euclidea della differenza tra $x$ e~$y$.  
Tra le funzioni più utilizzate ad esempio ci sono le funzioni di Wendland con differenti regolarità e con supporto compatto; oppure la ben nota funzione gaussiana e l'inversa multiquadrica.
In generale si usano versioni scalate $\phi_\varepsilon$ di queste funzioni, definite tramite un parametro di forma~$\varepsilon$.



Dato uno spazio nativo e un insieme~$X$ di locazioni, si può considerare il funzionale che esprime l'{\em errore puntuale} di interpolazione in~$y$, cioè quel funzionale che associa a ogni funzione~$f$ dello spazio nativo la differenza tra i valori in $y$ di $f$ e della sua interpolante sull'insieme~$X$ di locazioni.  A partire da questo funzionale, considerando la sua norma operatoriale, si definisce la {\em funzione potenza} associata all'insieme di locazioni~$X$.  La funzione potenza dipende solamente dalle locazioni e dal kernel, ed è una quantità esplicitamente calcolabile.  Essa assume valori positivi e limitati superiormente, e tali valori dipendono dalla posizione del punto considerato rispetto all'insieme di locazioni.

Quando si considera un insieme $X$ di locazioni e a tale insieme si aggiunge un nuovo punto $y$, la norma dell'interpolante aumenta.  L'incremento della norma tiene in considerazione non solo la differenza nel punto~$y$ tra il valore reale della funzione~$f$ e il valore della sua interpolante definita dalle locazioni~$X$, ma anche la posizione di $y$ rispetto all'insieme di punti~$X$, tramite la funzione potenza.  Perciò la differenza tra i quadrati delle norme delle interpolanti indica quanto accuratamente il valore di~$f$ nel punto~$y$ viene previsto dalla funzione che interpola i dati sulle locazioni~$X$: ci si aspetta infatti che la previsione sia più accurata per punti $y$ vicini alle locazioni~$X$, rispetto a punti~$y$ distanti da esse.
Se $f$ appartiene allo spazio nativo del kernel, allora la  norma  dell’interpolante è limitata dalla norma di $f$.  Se l’insieme di locazioni è già abbastanza numeroso per cui la differenza tra la norma di $f$ e della sua interpolante è piccola,  un eccessivo aumento della norma dell’interpolante quando si aggiunge un nuovo punto indica che il nuovo dato vìola il modello corrente, ossia che l’insieme complessivo dei dati non proviene da una singola funzione $f$ nello spazio nativo del kernel considerato.


Per vedere se tutti i punti provengono da uno stesso modello possiamo dunque effettuare un processo di {\em leave-one-out cross-validation}, osservando la variazione della norma dell’interpolante quando si rimuove ciascun punto per volta dall’insieme di dati.  Consideriamo dunque il vettore~$U$ la cui $k$-esima componente~$U_k$ è definita come la differenza tra il quadrato della norma dell’interpolante su tutti i dati e il quadrato della norma dell’interpolante definita dall’insieme di dati privato del $k$-esimo punto.  Il termine $U_k$, per quanto detto prima, indica quanto accuratamente il valore di $f$ nel punto~$x_k$ viene previsto dalla funzione che interpola $f$ sulle altre locazioni.  Un valore elevato di~$U_k$ segnala dunque che il $k$-esimo dato devìa molto dal modello descritto dagli altri dati.  Ad esempio, nel caso di un campione di dati proveniente da una funzione discontinua, i dati vicini alle discontinuità della funzione avranno un corrispondente valore elevato nel vettore~$U$. 


Se $U$ viene calcolato in base alla sua definizione, il costo computazionale risulta essere elevato, poiché è necessario risolvere $N$ sistemi lineari di dimensione~$N{-}1$.
Tuttavia si dimostra che esiste una formula più efficiente per calcolare~$U$.  Infatti $U$ si può ottenere considerando solo la matrice di interpolazione dell’insieme completo di locazioni, dunque operando con una sola matrice di dimensione~$N$ anziché $N$ matrici distinte di dimensione~$N{-}1$. 
Invece che considerare semplicemente gli incrementi assoluti dei quadrati delle norme, è opportuno considerare i loro incrementi relativi, dividendo ciascun termine del vettore~$U$ per il quadrato della norma della funzione che interpola tutti i dati.  Mantenendo il costo computazionale pressoché invariato, si ottiene così un nuovo vettore~$Q$.


D’ora in poi consideriamo solo kernel radiali, descritti da una funzione~$\phi$ e un parametro di forma~$\varepsilon$.
 Sotto questa ipotesi il  vettore $Q$  soddisfa le seguenti proprietà:
\begitems
* $Q$ converge a un vettore limite~$\widehat Q$ quando $\varepsilon$ tende a zero.
* Il vettore limite $\widehat Q$ non varia se si applica una trasformazione lineare non-nulla alla funzione~$f$ o equivalentemente all’insieme di valori campionati.
* $\widehat Q$ non varia inoltre se si applica una similitudine all’insieme di locazioni~$X$.
\enditems
Quindi l’informazione contenuta nelle componenti di~$\widehat Q$ dipende solo dalla configurazione dei dati, ma non dai loro effettivi valori.
Per analizzare il comportamento di $Q$ quando $\varepsilon$ tende a zero, e quindi per dimostrare le proprietà elencate, lo strumento principale che si è utilizzato è lo sviluppo in serie di Laurent dell’inversa della matrice di interpolazione.


Dal punto di vista operativo si osserva che una buona approssimazione $\tilde Q$ del limite di~$Q$ si può ottenere semplicemente utilizzando un valore di $\varepsilon$ opportunamente piccolo, senza incorrere in problemi di malcondizionamento, quando la funzione $\phi$ considerata ha regolarità bassa, ad esempio nel caso della funzione di Wendland ${\cal C}^0$ o ${\cal C}^2$.
È possibile definire un classificatore di dati $\cal R$ che, confrontanto la somma delle componenti di $\tilde Q$ con una certa soglia $\tau$, stabilisce se l’insieme di dati è “regolare”, ossia se proviene da un unico modello appartenente allo spazio nativo del kernel considerato.  
 La soglia di decisione~$\tau$ rappresenta l’evidenza richiesta per la classificazione:  più il valore scelto per~$\tau$ è piccolo, maggiore è l’evidenza richiesta affinché il classificatore~$\cal R$ etichetti un campione di dati come “regolare”.  Grazie alle proprietà di invarianza per similitudini sulle locazioni e sui valori campionati soddisfatte dal limite del vettore~$Q$, il valore di~$\tau$ {\em non} deve essere scelto in funzione dei dati che si vogliono classificare.


%Spiego ora come il vettore~$\widehat Q$ e il classificatore~$\cal R$ appena definito possono essere utilizzati per  ricostruire una funzione discontinua a partire da dati sparsi.

Ritornando al problema iniziale --- cioè di ricostruire una funzione discontinua a partire da dati sparsi --- spiego ora come il vettore~$\tilde Q$ e il classificatore~$\cal R$ appena definito possono essere impiegati per raggruppare le locazioni~$X$ in base al loro sottodominio di appartenenza.
Il classificatore~$\cal R$ si utilizza per determinare se un certo sottoinsieme di punti,  proviene da un solo modello, e quindi è tutto contenuto in un solo sottodominio~$\Omega_j$.
Analizzando localmente i dati in~$\Omega$ si cerca di estendere l’insieme di dati che si sta considerando fino ad includere tutti quelli appartenenti allo stesso dominio.
Quando si trova un sottoinsieme locale di punti che non proviene dallo stesso modello, si guardano le singole componenti di $\tilde Q$, in particolare quella di valore massimo, per determinare quali sono i punti che appartengono a un dominio differente da quello che si sta considerando.


Mostro ora un esempio concreto, in cui si applica la tecnica descritta.  In un insieme di dati sparsi nel quadrato unitario  si è campionata una funzione che è discontinua lungo la curva qui rappresentata.  Il dominio risulta diviso in due sottodomini che devono essere determinati a partire dai dati.
Per poter analizzare localmente il dominio, si utilizza la triangolazione di Delaunay delle locazioni.  Si seleziona casualmente un triangolo, e quindi il suo circocentro.   Si considera successivamente l’insieme dei $k$ punti più vicini al circocentro, dove $k$ è un numero  fissato.  Tramite il classificatore si stabilisce se l’insieme di dati considerato è “regolare”, ossia se tutti i dati appartengono allo stesso sottodominio.   In questo caso ciò si verifica, e quindi i punti vengono utilizzati per iniziare a costruire il sottodominio.  I triangoli verdi sono quelli formati dai punti considerati, mentre i triangoli gialli sono quelli di bordo.  A partire da un triangolo di bordo scelto casualmente si ripete la stesso identico procedimento, includendo così nel dominio altri punti appartenenti allo stesso modello.  Si procede per estensioni successive finché non ci si imbatte in un insieme di punti che viene classificato come “non regolare”, poiché include dati provenienti da due modelli differenti.  A questo punto, utilizzando informazioni sia sul sottodominio finora costruito, sia sulle componenti del vettore~$\tilde Q$, si rimuovono punti che presumibilmente non appartengono al modello considerato, finché non si ottiene un sottoinsieme di punti che viene etichettato come “regolare”, e lo si ingloba quindi nel dominio.  Proseguendo successivamente alla stessa maniera, ci si può imbattere in un ultimo caso, quello in cui il triangolo considerato ha vertici appartenenti  a sottodomini differenti.  In questo caso non esiste alcun sottoinsieme di dati “regolare” contenente tutti i vertici del triangolo, quindi si conclude che il triangolo si trova al confine di due sottodomini, e lo si colora di rosso.   
Proseguendo in questa maniera si arriva ad una situazione di questo tipo.
%Proseguendo ulteriormente finché non si sono esauriti i triangoli gialli di bordo, e successivamente ripetendo l’intero procedimento su i triagoli non ancora analizzati, si arriva ad una situazione di questo tipo.

A sinistra è visualizzato il grafico della funzione campionata.
Nella triangolazione, i domini due domini connessi di triangoli verdi racchiudono tramite i loro vertici i punti dei due sottoinsiemi di locazioni che si volevano individuare.   Come conseguenza inoltre, tramite la fascia di triangoli rossi si localizza anche la curva di discontinuità.\nl
In quest’altro esempio la curva di discontinuità è una curva chiusa e il salto di discontinuità varia lungo la curva.  Si vede che i due domini vengono comunque correttamente separati.\nl 
Quest’ultimo esempio mostra che la tecnica descritta si applica anche nel caso siano presenti più curve di discontinuità.  In tutti e tre gli esempi si sono utilizzati gli stessi parametri, in particolare la stessa tolleranza $\tau$, che appunto non deve dipendere dalla funzione considerata.  Si è utilizzato il kernel radiale definito dalla funzione di Wendland di classe~${\cal C}^0$, in quanto esso ha regolarità sufficiente per individuare le discontinuità.
\nl
Come osservazione aggiungo  che utilizzando un kernel più regolare, come ad esempio quello definito dalla funzione di Wendland ${\cal C}^2$, oltre alle discontinuità della funzione stessa si possono individuare le discontinuità delle sue derivate.  In questo esempio si individua una discontinuità nel gradiente.



Col metodo descritto si riesce ad assegnare le locazioni ai loro rispettivi sottodomini.  Il passo successivo è quello di ottenere delle ricostruzioni~$\tilde\Omega_j$ di ciascun sottodominio~$\Omega_j$.  Infatti, supponendo di averle ottenute, poi la funzione campionata si può ricostruire su ciascun sottodominio separatamente interpolando esclusivamente i dati di quel sottodominio.
Per effettivamente ottenere le ricostruzioni~$\tilde\Omega_j$ a partire dai sottoinsiemi di locazioni~$X_j$ si può utilizzare il modello di machine learning chiamato {\em support vector machine}.



Nella loro versione base le SVM sono modelli binari che permettono di separare una coppia di insiemi di punti.  Tuttavia esse si possono estendere a modelli multiclasse che permettono di separare un numero arbitrario di insiemi di punti.
Per la descrizione mi limito al caso binario.
Innanzitutto si trasformano i punti di ciascuno dei due sottoinsiemi in punti appartenenti ad  uno spazio vettoriale~$V$ di dimensione superiore, tramite una funzione~$\Theta$, detta {\em feature map}.  
 Successivamente si determina un iperpiano nello spazio $V$ che separa i due insiemi di punti $\Theta(X_1)$ e $\Theta(X_2)$. 
L’iperpiano che si determina è espresso in forma implicita da una funzione~$h$.  La funzione $h$ viene utilizzata poi per classificare i punti del dominio~$\Omega$, determinando i due sottoinsiemi~$\tilde\Omega_1$ e $\tilde\Omega_2$.    
In questo modo, cioè trovando una curva di separazione lineare nello spazio~$V$, si ottiene una curva di separazione possibilmente non lineare nello spazio di partenza.


L’perpiano viene determinato massimizzando il margine geometrico, tra i due insiemi di dati, in senso stretto se i due insiemi sono linearmente separabili, o in un senso debole se non lo sono.  In ogni caso si deve risolvere un problema ottimizzazione vincolata, che dipende dai dati originali o direttamente o  mediante prodotti scalari dei dati trasformati tramite~$\Theta$.   
Per calcolare efficientemente questi prodotti scalari, in caso $V$ abbia dimensione elevata, si sfrutta il teorema di Mercer.


Esso afferma che ogni kernel $K$ continuo simmetrico e definito positivo si può decomporre per mezzo di una funzione~$\Theta$, che si determina considerando le autofunzioni di un certo operatore integrale associato a~$K$.  Perciò, se come feature map si sceglie la funzione~$\Theta$ che decompone un kernel noto, allora il calcolo dei prodotti scalari dei dati trasformati tramite~$\Theta$ si riduce alla semplice valutazione del kernel nei dati originali.  Inoltre, in questo modo, per poter definire il modello SVM non è necessario conoscere esplicitamente la feature map utilizzata, ma è sufficiente conoscere soltanto il suo kernel associato.  Uno dei kernel più utilizzati, ad esempio, è il kernel gaussiano, per il quale è noto che lo spazio di arrivo della feature map ha dimensione infinita.

Concludo quindi mostrando la ricostruzione delle funzioni precedentemente considerate. In alto a sinistra è visualizzato il grafico della funzione campionata, e sotto di essa il grafico della funzione ricostruita.  Inoltre nel dominio in blu è visualizzata la curva di discontinuità reale, e in verde quella ricostruita tramite una SVM con kernel gaussiano.\nl
Questa è la ricostruzione della seconda funzione.  In entrambi i casi il fenomeno di Gibbs non si manifesta e la curva di discontinuità risulta ben delineata nella ricostruzione.\nl
Infine per completezza mostro anche il risultato per il terzo esempio considerato.

Questi sono i principali riferimenti bibliografici utilizzati.  Vi ringrazio per l’attenzione.



\bye
