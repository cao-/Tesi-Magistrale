

In the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\Omega\times\Omega\to\R$ the interpolation problem has solution for each set~$X$ of data sites, since the native space contains by construction all the subspaces~$\Cal S_{Φ,\. X}$.  If $f$ is an arbitrary function of~$\,\R^\Omega$, in general not very much can be said about the difference (or error) between $f$ and its unique interpolant from~$\Cal S_{Φ,\. X}$; instead, if $f$ comes from the native space~$\Cal N_Φ$, then some estimates on the interpolation error can actually be derived, as we are going to see.

As usual, let $X = \{\range x_N/\}$ be the set of locations, and $S_{Φ,\. X}$ the interpolating space generated by the basis
$$
\Cal B = \{Φ(\cdot, x_1), Φ(\cdot, x_2),\dots, Φ(\cdot, x_N)\}\mathbox.
$$
Since the interpolation matrix~$\Bm A\coloneq \Bm A_{Φ,\. X}$ defined in~\ref[intmat] is invertible, we can consider its {\em inverse}~$\Bm C$, which is the $N\times N$~matrix such that
$$
\Bm A\Bm C=\Bm I\mathbox,
$$
with $\Bm I$ denoting the $N\times N$ identity matrix.  (For simplicity of notation, like in this case, we will often omit the subscripts $Φ$ or $X$ if there is no need to stress the dependence from the kernel or the data sites respectively.) Then, the columns~$\{\Bm c^{(h)}\}_{h\in\{1,2,\dots, N\}}$ of the matrix~$\Bm C$ are linearly independent and therefore can be used to generate a new basis
$$
\Cal U=\{\range u_N/\}
$$
for the space~$\Cal S_{Φ,\. X}$.  Specifically, we can define the new basis functions~$\{u_h\}_{h\in\{1,2,\dots, N\}}$ by
$$
u_h\coloneq \sum_{k = 1}^N c^{(h)}_k\Phi(\cdot, x_k)\mathbox. \eqmark[card]
$$
These basis functions, called {\em cardinal} functions, have the property that
$$
u_h(x_j) = \cases{$1$,& if $j = h$ \cr
					             $0$,& if $j \neq h$,}
$$
since by definition the value of~$u_h$ at each point~$x_j$ is obtained by multiplying the $j$-th~row of~$\Bm A$ by the $h$-th~column of its inverse~$\Bm C$.  In the cardinal basis~$\Cal U$  the interpolant~$s_{X,f}$ of a function~$f\in\Cal N_Φ$ on the set of locations~$X$  is expressed in the simple  {\em Lagrangian form}
$$
s_{X,f} = \sum_{h = 1}^N f(x_h)\, u_h,
$$
that uses the data values $f(x_1), f(x_2),\dots f(x_N)$  as coefficients.  This form of expressing the interpolant, which keeps the data values separated from the data sites, is the starting point to obtain an error estimate for the interpolation error.

We now define, for each point~$x\in\Omega$, the {\em error functional}~$\epsilon_x:\cal N_Φ\to\R$  by
$$
\epsilon_x(f) = f(x)-s_{X,f}(x)\mathbox{, \quad$f\in\Cal N_Φ$.}
$$
For each~$f\in\Cal N_Φ$ this functional evaluates at the point~$x$ exactly the quantity that we want to estimate, that is the difference between $f$ and its interpolant~$s_{X, f}$. By using the point-evaluation functionals and the Lagrangian form of the interpolant, the error functional can be expressed as
$$
\epsilon_x = \delta_x-\sum_{h=1}^N u_h(x)\,\delta_{x_h}, \eqmark[errfun]
$$
which implies in particular the $\epsilon_x$ belongs to the topological dual~$\Cal N_Φ^*$ of the native space.
We are now ready to define a function which assumes a very important role in the error theory for native spaces.

\preskip
\label[powerdef]
\definition The {\em power function}~$P_X$ (more properly, $P_{Φ,\. X}$) associated to the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\,\Omega\times\Omega\to\R$ and a set of locations~$X\subset\Omega$ is the function which maps each point $x\in\Omega$ to the operator norm of the error functional~$\epsilon_x$, i.e., the function whose expression is
$$
P_X(x) \coloneq \opnorm{\epsilon_x}_{\Phi}\coloneq \sup_{f\neq 0}{|\epsilon_x(f)|\over\phantom{{}_Φ}\norm{\.f}_Φ}\mathbox{,\quad $x\in\Omega$.}
$$
\postskip

\noindent The power function~$P_X$ is a well defined real-valued function, since the operator norm of a continuous (or, equivalently, bounded) linear functional has finite value.

The error functional~$\epsilon_x\in\Cal N_Φ^*$ is represented in~$\Cal N_Φ$, in the sense of the Riesz representation theorem (see Stein~\cite[stein_2009]), by the function
$$
Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h).
$$
In fact, from expression~\ref[errfun], we obtain that, for each~$f\in\Cal N_Φ$,
$$
\eqalign{\epsilon_x(f) &= \delta_x(f)-\sum_{h=1}^N u_h(x)\,\delta_{x_h}(f)  \cr
				      &= \form{Φ(\cdot,x)}f_Φ-\sum_{h=1}^Nu_h(x)\form{Φ(\cdot,x_h)}f_Φ \cr
				      &= \form{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}f_Φ.
}
$$
Since the operator norm of a functional in the dual of a Hilbert space is equal to the Hilbert space norm of its representative, it is possible to express the power function also~as
$$
P_X(x) = \opnorm{\epsilon_x}_Φ = \norm{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ.
$$
This norm can be actually computed as in the following steps,
$$
\eqalign{P_X(x) &= \norm{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ  \cr
                                    &= \sqrt{\norm{Φ(\cdot,x)}_Φ^2 -2\, \bform{Φ(\cdot,x)}{\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ + \bnorm{\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ^2} \cr
                                    &= \sqrt{Φ(x,x) - 2\sum_{h=1}^Nu_h(x)\,Φ(x,x_h) + \sum_{h=1}^N\sum_{k=1}^N u_h(x) u_k(x)\,Φ(x_h, x_k)}.
}
$$ 
 If we define the  two vectors $\Bm u(x)$ and~$\Bm b(x)$ as
 $$
\eqalign{\Bm u(x)&\coloneq (u_1(x),u_2(x),\dots, u_N(x))^T \cr
\Bm b(x)&\coloneq (Φ(x,x_1), Φ(x,x_2),\dots,Φ(x,x_N))^T,}
 $$
 then the expression for the power function can be rewritten as
 $$
 \eqalign{P_X(x) &= \sqrt{Φ(x,x) -2\Bm u(x)^T\Bm b(x) + \Bm u(x)^T\!\Bm A\,\Bm u(x)}  \cr
 			    &= \sqrt{\pmatrix{\Bm u(x) \cr -1}^{\rlap{$\scriptstyle\!T$}}
       		     			   \pmatrix{\Bm A\; & \Bm b(x) \cr \Bm b(x)^T & Φ(x,x)}
         				 	  \pmatrix{\Bm u(x) \cr -1}}\,.} \eqmark[pow1]
$$
 From the definition of~$u_h$ in expression~\ref[card] it follows that~$u_h(x) = (\Bm c^{(h)})^T\Bm b(x)$, and so that~$\Bm u(x) = \Bm C^{\,T}\Bm b(x) =  (\Bm A^{-1})^T\, \Bm b(x) = \Bm A^{-1} \Bm b(x)$.
 This allows us finally to find a computable expression for~$P_X(x)$, that is
 $$
 \eqalignno{P_X(x) &=\sqrt{ Φ(x,x) -2\Bm u(x)^T\!\Bm A\,\Bm u(x) + \Bm u(x)^T\!\Bm A\,\Bm u(x)} \cr
 			    &= \sqrt{ Φ(x,x) -\Bm u(x)^T\!\Bm A\,\Bm u(x)}  &\eqmark[pow2]\cr
 			     &= \sqrt{Φ(x,x)-\Bm b(x)^T\Bm A^{-1}\Bm b(x)}.
}
$$

These expressions let us deduce some properties of the power function.  
From~\ref[pow2] it follows that
$$
0\leq P_X(x) \leq \sqrt{Φ(x,x)},
$$
 because of the positive definiteness of the matrix~$\Bm A$.  Additionally, the evaluation of $P_X$ at the data sites gives
 $$
\eqalign{P_X(x_j) &= \sqrt{Φ(\smash{x_j}, \smash{x_j}) - \Bm u(\smash{x_j})^T\!\Bm A\,\Bm u(\smash{x_j})} \cr
			      &= \sqrt{Φ(\smash{x_j}, \smash{x_j})- \smash{A_{j,j}}} = 0.}

 $$
 Expression~\ref[pow1] instead implies that  $P_X(x) > 0$ if $x\notin X$. In fact, in this case the matrix
 $$
 \pmatrix{\Bm A\; & \Bm b(x) \cr
 		 \Bm b(x)^T & \Phi(x,x)}
 $$
 is positive definite because it is the interpolation matrix~$\Bm A_{Φ,\.X\cup\{x\}}$ that arises from the augmented set of locations~$X\cup\{x\}$.
 


The power function therefore is a function which depends only on the kernel and the data sites; it has value zero at the data sites and elsewhere is strictly positive and bounded above by the function~$x\mapsto\sqrt{Φ(x,x)}$.  If the kernel~$Φ$ is continuous, then also $P_X$ is continuous; if the kernel is translational invariant (for instance, a radial kernel), then $P_X$ is bounded above by a constant, because  in this case $Φ(x,x)$ does not depend on~$x$.



Now define for each~$x\in\Omega$ the {\em quadratic form}~$Q_x:\R^N\to\R$ by
$$
Q_x(\Bm v) \coloneq Φ(x,x) - 2\Bm v^T \Bm b(x) + \Bm v^T\! \Bm A \.\Bm v = \norm{Φ(\cdot,x)-\sum_{h=1}^N v_h\,Φ(\cdot,x_h)}^2_Φ.
$$
The positive definiteness of~$\Bm A$ implies that~$Q_x$ is strictly convex, and so that $Q_x$ has a unique point of minimum.  The  vector that realises the minimum of~$Q_x$ turns out to be~$\Bm u(x)$. In fact, the gradient of~$Q_x$, which has value
$$
\mathop{\rm grad}\nolimits Q_x(\Bm v) = -2\Bm b(x) + 2\Bm A \Bm v,
$$
is zero exactly at $\Bm u(x)$.
The combination of this property of~$Q_x$  with expression~\ref[pow1] produces an improved pointwise upper bound for the power function, that is
$$
P_X(x) = \sqrt{Q_x(\Bm u(x))} < \sqrt{Q_x(\Bm v)},\quad\mathbox{for each~$\Bm v\neq \Bm u(x)$.}\eqmark[pow3]
$$

 
 
 An immediate consequence of the definition of the power function that we gave in terms of the error functional (definition~\ref[powerdef]) is the following result.

\preskip
\theorem
If a function~$f:\Omega\to\R$ belongs to the native space of a symmetric positive definite kernel~$Φ$, and $X\subset\Omega$ is a set of data sites, then for each~$x\in\Omega$ the difference between the values at~$x$ of $f$ and its interpolant~$s_{X,f}\in\Cal S_{Φ,\. X}$ is estimated by
$$
|\.f(x)-s_{X,f}(x)|\leq P_X(x)\,\norm{\.f}_Φ.
$$

\proof
If $f=0$, then also $s_{X,f}=0$ and the inequality is trivially satisfied; otherwise, simply because of how the operator norm is defined, we have that
$$
\opnorm{\epsilon_x}_Φ\geq {|\epsilon_x(f)| \over\phantom{{}_Φ} \norm{\.f}_Φ}
$$
and hence that
$$
|\epsilon_x(f)|\leq\opnorm{\epsilon_x}_Φ\,\norm{\.f}_Φ.
$$
Finally, the stated result follows from the definitions of~$\epsilon_x$ and~$P_X$. \hfill\QED
\postskip










\noindent The importance of this theorem consists in the fact that it decouples the data sites and the data values, since it splits the error into two factors:  the factor~$P_X(x)$ which depends only on the  data sites  (and the kernel); the factor~ $\norm f_Φ$ which depends only on the function~$f$ that produced the data values (and on the kernel).  It is then possible to study the dependence of the interpolating error on the two factors  separately.  If for instance we study the behaviour of the power function for a fixed set of locations, then we easily can produce estimates for the interpolation error on those data sites for any function coming from the native space.



Starting from inequality~\ref[pow3] it is possible to derive an upper bound for the power function in terms of how well the data sites~$X\subset\Omega$ fill the domain~$\Omega$, which is measured by the {\em fill distance}~$h_X$, defined as
$$
h_X = \sup_{x\in\Omega}\min_{x_j\in X}\norm{x-x_j}.
$$
If $\Omega$ is bounded and satisfies an interior cone condition,\fnote{A domain~$\Omega\in\R^d$ satisfies an {\em interior cone condition} if there exists an angle~$\theta\in(0,\pi/2)$ and a radius~$r>0$ such that for every~$x\in\Omega$ there exists a unit vector~$\xi(x)$ such that the cone
\vskip-\parskip
\vskip-1.5ex
$$
C = \{x+\lambda y:\, y\in\R^d,\, \norm y = 1,\, y^T\xi(x)\geq\cos\theta,\, \lambda\in[0,r]\}
$$
\vskip-\parskip
\vskip-1ex \noindent
is contained in~$\Omega$.} then 
$$
P_X^2(x)\leq F(h_X) \quad \mathbox{for all $x\in\Omega$,}
$$
for some function~$F$ which goes to zero as~$h_X\to 0$ (Wendland~\cite[wendland_2004], chapter~11).  The function~$F$ depends on the kernel of the native space, and can be explicitly computed.  In table~\ref[pow-tab] we report the known bounds on the power function for some of the most used radial kernels.

\label[pow-tab]
\topinsert
\centerline{
\table{lcc}{\crl
                        & $\phi(r)$               & $F(h)$  \crl\tskip2pt
Gaussians               & $e^{-\epsilon r^2}$,\quad $\epsilon>0$        & $e^{-c\,|\!\log h|/h}$,\quad  $c>0$ \cr\tskip2pt
Inverse multiquadrics   & $(1+r)^{-\epsilon}$,\quad $\epsilon>0$ & $e^{-c/h}$,\quad  $c>0$ \cr \tskip2pt
Wendland's functions    & $\phi_{d,k}(r)$           & $h^{2k+1}$ \cr\tskip5pt
\crl
}}
\caption/t Upper bounds on~$P_X^2$ as functions~$F(h)$ of the fill distance~$h$. The functions $F$ are given only up to a constant that does not depend on the data sites.  They express the decay rate of the power function as the fill distance approaches zero.
\endinsert




