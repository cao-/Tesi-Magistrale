

The native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\Omega\times\Omega\to\R$ allows to solve the interpolation problem for each set~$X$ of data sites, since it contains by construction all the subspaces~$\Cal S_{Φ,\. X}$.  If $f$ is an arbitrary function of~$\,\R^\Omega\!\!\!$,\,\, in general not very much can be said about the difference (or error) between $f$ and its unique interpolant from~$\Cal S_{Φ,\. X}$; instead, if $f$ comes from the native space~$\Cal N_Φ$, then some estimates on the interpolation error can actually be derived, as we are going to see.

As usual, let $X = \{\range x_N/\}$ be the set of locations, and $S_{Φ,\. X}$ the interpolating space generated by the basis
$$
\Cal B = \{Φ(\cdot, x_1), Φ(\cdot, x_2),\dots, Φ(\cdot, x_N)\}\mathbox.
$$
Since the interpolation matrix~$\Bm A\coloneq \Bm A_{Φ,\. X}$ defined in~\ref[intmat] is invertible, we can consider its {\em inverse}~$\Bm C$, which is the $N\times N$~matrix such that
$$
\Bm{AC}=\Bm{CA}=\Bm I,
$$
with $\Bm I$ denoting the $N\times N$ identity matrix (for simplicity of notation, like in this case, we will henceforth omit the subscripts $Φ$ or $X$ if there is no need to stress the dependence from the kernel or the data sites respectively). Then, the columns~$\{\Bm C^{\.[h]}\}_{h\in\{1,2,\dots, N\}}$ of the matrix~$\Bm C$ are linearly independent and therefore can be used to generate a new basis
$$
\Cal U=\{\range u_N/\}
$$
for the space~$\Cal S_{Φ,\. X}$.  Specifically, we can define the new basis functions~$\{u_h\}_{h\in\{1,2,\dots, N\}}$ by
$$
u_h\coloneq \sum_{k = 1}^N C^{[h]}_k\Phi(\cdot, x_k)\mathbox. \eqmark[card]
$$
These basis functions, called {\em cardinal functions}, have the property that
$$
u_h(x_j) = \cases{$1$,& if $j = h$ \cr
					             $0$,& if $j \neq h$,}
$$
since by definition the value of~$u_h$ at each point~$x_j$ is obtained by multiplying the $j$-th~row of~$\Bm A$ by the $h$-th~column of its inverse~$\Bm C$.  In the cardinal basis~$\Cal U$  the interpolant~$s_{X,f}$ of a function~$f\in\Cal N_Φ$ on the set of locations~$X$  is expressed in the simple  {\em Lagrangian form}
$$
s_{X,f} = \sum_{h = 1}^N f(x_h)\, u_h,
$$
that uses the data values $f(x_1), f(x_2),\dots f(x_N)$  as coefficients.  This form of expressing the interpolant, which keeps the data values separated from the data sites, is the starting point to obtain an error estimate for the interpolation error.

We now define, for each point~$x\in\Omega$, the {\em error functional}~$\epsilon_x:\cal N_Φ\to\R$  by
$$
\epsilon_x(f) = f(x)-s_{X,f}(x)\mathbox{, \quad$f\in\Cal N_Φ$.}
$$
For each~$f\in\Cal N_Φ$ this functional evaluates at the point~$x$ exactly the quantity that we want to estimate, that is the difference between $f$ and its interpolant~$s_{X, f}$. By using the point-evaluation functionals and the Lagrangian form of the interpolant, the error functional can be expressed as
$$
\epsilon_x = \delta_x-\sum_{h=1}^N u_h(x)\,\delta_{x_h}, \eqmark[errfun]
$$
which implies in particular the $\epsilon_x$ belongs to the topological dual~$\Cal N_Φ^*$ of the native space.
We are now ready to define a function which assumes a very important role in the error theory for native spaces.

\preskip
\label[powerdef]
\definition The {\em power function}~$P_X$ (more properly, $P_{Φ,\. X}$) associated to the native space~$\Cal N_Φ$ of a symmetric positive definite kernel~$Φ:\,\Omega\times\Omega\to\R$ and a set of locations~$X\subset\Omega$ is the function which maps each point $x\in\Omega$ to the operator norm of the error functional~$\epsilon_x$, i.e., the function whose expression is
$$
P_X(x) \coloneq \opnorm{\epsilon_x}_{\Phi}\coloneq \sup_{f\neq 0}{|\epsilon_x(f)|\over\phantom{{}_Φ}\norm{\.f}_Φ}\mathbox{,\quad $x\in\Omega$.}
$$
\postskip

\noindent The power function~$P_X$ is a well defined positive real-valued function, since the operator norm of a continuous (or, equivalently, bounded) linear functional has finite value.

The error functional~$\epsilon_x\in\Cal N_Φ^*$ is represented in~$\Cal N_Φ$, in the sense of the Riesz representation theorem (see Stein and Shakarchi~\cite[stein-shakarchi_2009]), by the function
$$
Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h).
$$
In fact, from expression~\ref[errfun], we obtain that, for each~$f\in\Cal N_Φ$,
$$
\eqalign{\epsilon_x(f) &= \delta_x(f)-\sum_{h=1}^N u_h(x)\,\delta_{x_h}(f)  \cr
				      &= \form{Φ(\cdot,x)}f_Φ-\sum_{h=1}^Nu_h(x)\form{Φ(\cdot,x_h)}f_Φ \cr
				      &= \form{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}f_Φ.
}
$$
Since the operator norm of a functional in the dual of a Hilbert space is equal to the Hilbert space norm of its representative, it is possible to express the power function also~as
$$
P_X(x) = \opnorm{\epsilon_x}_Φ = \norm{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ.
$$
This norm can be actually computed as in the following steps,
$$
\eqalign{P_X(x) &= \norm{Φ(\cdot,x)-\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ  \cr
                                    &= \sqrt{\norm{Φ(\cdot,x)}_Φ^2 -2\, \bform{Φ(\cdot,x)}{\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ + \bnorm{\sum_{h=1}^Nu_h(x)\,Φ(\cdot,x_h)}_Φ^2} \cr
                                    &= \sqrt{Φ(x,x) - 2\sum_{h=1}^Nu_h(x)\,Φ(x,x_h) + \sum_{h=1}^N\sum_{k=1}^N u_h(x) u_k(x)\,Φ(x_h, x_k)}.
}
$$ 
 If we define the  two vectors $\Bm u(x)$ and~$\Bm t(x)$ as
 $$
\eqalign{\Bm u(x)&\coloneq (u_1(x),u_2(x),\dots, u_N(x))^T \cr
\Bm t(x)&\coloneq (Φ(x,x_1), Φ(x,x_2),\dots,Φ(x,x_N))^T,} \eqmark[uandt]
 $$
 then the expression for the power function can be rewritten as
 $$
 \eqalign{P_X(x) &= \sqrt{Φ(x,x) -2\Bm u(x)^T\Bm t(x) + \Bm u(x)^T\!\Bm A\,\Bm t(x)}  \cr
 			    &= \sqrt{\pmatrix{\Bm u(x) \cr -1}^{\rlap{$\scriptstyle\!T$}}
       		     			   \pmatrix{\Bm A\; & \Bm t(x) \cr \Bm t(x)^T & Φ(x,x)}
         				 	  \pmatrix{\Bm u(x) \cr -1}}\,.} \eqmark[pow1]
$$
 From the definition of~$u_h$ in expression~\ref[card] it follows that~$u_h(x) = (\Bm C^{\.[h]})^T\Bm t(x)$, and so that~$\Bm u(x) = \Bm C^{\,T}\Bm t(x) =  (\Bm A^{-1})^T\, \Bm t(x) = \Bm A^{-1} \Bm t(x)$.
 This allows us finally to find a computable expression for~$P_X(x)$, that is
 $$
 \eqalignno{P_X(x) &=\sqrt{ Φ(x,x) -2\Bm u(x)^T\!\Bm A\,\Bm u(x) + \Bm u(x)^T\!\Bm A\,\Bm u(x)} \cr
 			    &= \sqrt{ Φ(x,x) -\Bm u(x)^T\!\Bm A\,\Bm u(x)}  &\eqmark[pow2]\cr
 			     &= \sqrt{Φ(x,x)-\Bm t(x)^T\Bm A^{-1}\Bm t(x)}.
}
$$

These expressions let us deduce some properties of the power function.  
From~\ref[pow2] it follows that
$$
0\leq P_X(x) \leq \sqrt{Φ(x,x)},
$$
 because of the positive definiteness of the matrix~$\Bm A$.  Additionally, the evaluation of $P_X$ at the data sites gives
 $$
\eqalign{P_X(x_j) &= \sqrt{Φ(\smash{x_j}, \smash{x_j}) - \Bm u(\smash{x_j})^T\!\Bm A\,\Bm u(\smash{x_j})} \cr
			      &= \sqrt{Φ(\smash{x_j}, \smash{x_j})- \smash{A_{j,j}}} = 0.}

 $$
 Expression~\ref[pow1] instead implies that  $P_X(x) > 0$ if $x\notin X$. In fact, in this case the matrix
 $$
 \pmatrix{\Bm A\; & \Bm t(x) \cr
 		 \Bm t(x)^T & \Phi(x,x)}
 $$
 is positive definite because it is the interpolation matrix~$\Bm A_{Φ,\.X\cup\{x\}}$ that arises from the augmented set of locations~$X\cup\{x\}$.
 


The power function therefore is a function which depends only on the kernel and the data sites; it has value zero at the data sites and elsewhere is strictly positive and bounded above by the function~$x\mapsto\sqrt{Φ(x,x)}$.  If the kernel~$Φ$ is continuous, then also $P_X$ is continuous; if the kernel is translational invariant (for instance, a radial kernel), then $P_X$ is bounded above by a constant, because  in this case $Φ(x,x)$ does not depend on~$x$. These properties are depicted in Figure~\ref[powfig].
\label[powfig]
\topinsert
\kern-.3cm
\bgroup
\typoscale[900/900]
\picw=1.2\hsize
\line{\hss\inkinspic{power2.pdf}\hbox{\kern1.5em}\hss}
\egroup
\kern-3ex
\caption/f Graph (blue line) of the power function~$P_X$ for a set~$X$ of $5$~randomly distributed points (red circles) on the real line (black dashed line).  We used for this example the native space generated by the Wendland's radial function~$\phi_{3,1}$ of Table~\ref[wentab]---other radial functions produce a similar plot.
We see that $P_X$ is continuous and has value zero at the data sites, and it is always positive and bounded above by the constant~$\phi_{3,1}(0)=1$.  Moreover, $P_X(x)$ gets close to the value $\phi_{3,1}(0)=1$ when $x$ is far from~$X$.  In fact, $\Bm t(x)$ is close to the zero vector when the distance between $x$ and~$X$ is big (the vector $\Bm t(x)$ is actually equal to the zero vector if $x$ is sufficiently distant from~$X$, because $\phi_{3,1}$ is compactly supported), and in this case expression~\ref[pow2] says that
$P_X(x)= \sqrt{\Phi(x,x) - \Bm t(x)^T\!\Bm A^{-1} \Bm t(x)}\approx \sqrt{\Phi(x,x)} = \sqrt{\phi_{3,1}(0)} = 1$.
\bigskip
\endinsert

Now define for each~$x\in\Omega$ the {\em quadratic form}~$Q_x:\R^N\!\to\R$ by
$$
Q_x(\Bm v) \coloneq Φ(x,x) - 2\Bm v^T \Bm t(x) + \Bm v^T\! \Bm A \.\Bm v = \norm{Φ(\cdot,x)-\sum_{h=1}^N v_h\,Φ(\cdot,x_h)}^2_Φ.
$$
The positive definiteness of~$\Bm A$ implies that~$Q_x$ is strictly convex, and so that $Q_x$ has a unique point of minimum.  The  vector that realises the minimum of~$Q_x$ turns out to be~$\Bm u(x)$. In fact, the gradient of~$Q_x$, which has value
$$
\mathop{\rm grad}\nolimits Q_x(\Bm v) = -2\Bm t(x) + 2\Bm A \Bm v,
$$
is zero exactly at $\Bm u(x)$.
The combination of this property of~$Q_x$  with expression~\ref[pow1] produces an improved pointwise upper bound for the power function, that is
$$
P_X(x) = \sqrt{Q_x(\Bm u(x))} < \sqrt{Q_x(\Bm v)},\quad\mathbox{for each~$\Bm v\neq \Bm u(x)$.}\eqmark[pow3]
$$

 
 
 An immediate consequence of the definition of the power function that we gave in terms of the error functional (Definition~\ref[powerdef]) is the following result.

\label[errorestimate]
\preskip
\theorem
If a function~$f:\Omega\to\R$ belongs to the native space of a symmetric positive definite kernel~$Φ$, and $X\subset\Omega$ is a set of data sites, then for each~$x\in\Omega$ the difference between the values at~$x$ of $f$ and its interpolant~$s_{X,f}\in\Cal S_{Φ, X}$ is estimated by
$$
|\.f(x)-s_{X,f}(x)|\leq P_X(x)\,\norm{\.f}_Φ.
$$

\proof
If $f=0$, then also $s_{X,f}=0$ and the inequality is trivially satisfied; otherwise, simply because of how the operator norm is defined, it is true that
$$
\opnorm{\epsilon_x}_Φ\geq {|\epsilon_x(f)| \over\phantom{{}_Φ} \norm{\.f}_Φ}
$$
and hence that
$$
|\epsilon_x(f)|\leq\opnorm{\epsilon_x}_Φ\,\norm{\.f}_Φ.
$$
Finally, the stated result follows from the definitions of~$\epsilon_x$ and~$P_X$.~\QED
\postskip



 The importance of this theorem consists in the fact that it decouples the data sites and the data values, since it splits the error into two factors:  the factor~$P_X(x)$ which depends only on the  data sites  (and the kernel); the factor~ $\norm f_Φ$ which depends only on the function~$f$ that produced the data values (and on the kernel).  It is then possible to study the dependence of the interpolating error on the two factors  separately.  If for instance we study the behaviour of the power function for a fixed set of locations, then we easily can produce estimates for the error relative to interpolation on those data sites for any function coming from the native space.


Starting from inequality~\ref[pow3] it is possible to derive an upper bound for the power function in terms of how well the data sites~$X\subset\Omega$ fill the domain~$\Omega$, which is measured by the {\em fill distance}~$h_X$, defined as
$$
h_X = \sup_{x\in\Omega}\min_{x_j\in X}\norm{x-x_j}.
$$
If $\Omega$ is bounded and satisfies an interior cone condition,\fnote{A domain~$\Omega\in\R^d$ satisfies an {\em interior cone condition} if there exists an angle~$\theta\in(0,\pi/2)$ and a radius~$r>0$ such that for every~$x\in\Omega$ there exists a unit vector~$\xi(x)$ such that the cone
$$
C = \{x+\lambda y:\, y\in\R^d,\, \norm y = 1,\, y^T\xi(x)\geq\cos\theta,\, \lambda\in[0,r]\}
$$
is contained in~$\Omega$.} then 
$$
P_X^2(x)\leq F(h_X) \quad \mathbox{for all $x\in\Omega$,}\eqmark[powbound]
$$
for some monotone function~$F$ such that~$\lim_{h\to 0} F(h) = 0$ (Wendland~\cite[wendland_2004], Chapter~11).  The function~$F$ depends on the kernel of the native space, and can be explicitly computed.  In Table~\ref[errortab] we report the known bounds of the power function for some of the most used radial kernels.

\label[errortab]
\topinsert
\Red
\centerline{
\table{lccc}{\crl
                        & $\phi(r)$               & $F(h)$   & $G(q)$\crl\tskip2pt
Gaussians               & $e^{-\epsilon r^2}$,\quad $\epsilon>0$        & $e^{-c\,|\!\log h|/h}$,\quad  $c>0$ & $q^{-d}e^{-M_d^2/(\epsilon q^2)}$ \cr\tskip2pt
Inverse multiquadrics   &  $(1+r)^{-\beta}$,\quad $\beta>0$ & $e^{-c/h}$,\quad  $c>0$ & $q^{\beta-(d-1)/2}e^{-12.76\, dc/q}$ \cr \tskip2pt
Wendland's functions    & $\phi_{d,k}(r)$           & $h^{2k+1}$ & $q^{2k+1}$ \cr\tskip5pt
\crl
}}
\caption/t Upper bounds on~$P_X^2$ as functions~$F(h)$ of the fill distance~$h$. The functions $F$ are given only up to a constant that does not depend on the data sites.  They express the decay rate of the power function as the fill distance approaches zero.
\endinsert


These error estimates expressed in terms of the fill distance of the data sites are purely theorical.  In fact, when dealing with an actual numerical interpolation performed on a calculator, one must take into account also the numerical {\em stability}\fnote{An algorithm is {\em stable} if its output results do not change much when the input data is modified slightly.  On a calculator some perturbations of the input data (and also of the intermediate results of the computations) are unavoidable when real numbers are stored slighly modified in the form of floating point numbers.} of the computations.  The interpolation matrix~$\Bm A = \Bm A_{Φ, X}$ of~\ref[intmat] has condition number that can be expressed as ratio of its maximum and minimum eigenvalues,
$$
\mathop{\rm cond}(A_{Φ, X})={λ_{\EBGaramond\mathbox{max}}(A_{Φ, X})\over λ_{\EBGaramond\mathbox{min}}(A_{Φ, X})}.
$$
It can be shown (Shaback~\cite[schaback_1997b], Wendland~\cite[wendland_2004], Chapter~12) that the numerator is well behaved, because it can grow at most proportionally to the number~$N$ of data sites,
$$
λ_{\EBGaramond\mathbox{max}}(A_{Φ, X}) ≤ N\.Φ(0,0),
$$
while the denominator can be bounded in the following way:
$$
G(q_x) ≤ λ_{\EBGaramond\mathbox{min}}(A) ≤ P_{X\setminus\{x_j\}}^2(x_j) ≤ F(h_{X\setminus\{x_j\}})\qquad\mathbox{for all~$x_j\in X$,}
$$
where $G$ is a positive monotone function dependent on~$Φ$ such that~$\lim_{q\to 0} G(q) = 0$,  and~$q_X$ is the {\em separation distance} of~$X$, defined as
$$
q_X = {1\over 2}\min_{i\neq j}\norm{x_i-x_j}.
$$
This means that the most straightforward algorithm to find the interpolant of a function~$f$ on a set~$X$ of data sites---the one that requires to compute the interpolation coefficients by solving the system~\ref[system] of the interpolation conditions---could be unstable if~$q_x$ is small.
We report in Table~\ref[errortab], next to the expressions for~$F$, the known expressions for $G$ for the same radial kernels.


Anyhow, even when the interpolation matrix~$A$ associated to the standard basis of kernel translates is badly conditioned, it is possible to stably compute the interpolant.  In fact, while computing the interpolation coefficients of the interpolant with respect to the standard basis is a badly conditioned problem, the computation of the interpolant itself is well conditioned (De~Marchi and Shaback~\cite[demarchi-shaback_2010]).  Some attempts at stably computing the interpolant by changing the basis of the interpolating space are provided by Pazouki and Schaback~\cite[pazouki-schaback_2011]; other two most notable algorithms are the Contour-Padé method (Fornberg and Wright~\cite[fornberg-wright_2004]) and the RBF-QR method (Fornberg, Larsson and Flyer~\cite[fornberg-larsson-flyer_2011]).

% See Shaback 1995

% mitigate

